#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams-bytype
theorems-sec-bytype
theorems-ams-extended-bytype
eqs-within-sections
figs-within-sections
tabs-within-sections
\end_modules
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format pdf2
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 1cm
\headsep 1cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 3
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{section}{1}
\end_layout

\end_inset


\end_layout

\begin_layout Section
A typical neural network for solving the MNIST database
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Abstract
The MNIST database (Mixed National Institute of Standards and Technology
 database) is a large database of handwritten digits that is commonly used
 for training various image processing systems.
 The database contains 60,000 training images and 10,000 testing images.
 There have been a number of scientific papers on attempts to achieve the
 lowest error rate; one paper, using a hierarchical system of convolutional
 neural networks, manages to get an error rate on the MNIST database of
 
\begin_inset Formula $0.23\,\%$
\end_inset

 (in a paper by the original creators of the database, they used a support
 vector machine to get an error rate of 
\begin_inset Formula $0.8\%$
\end_inset

).
\begin_inset Foot
status open

\begin_layout Plain Layout
Details from 
\begin_inset CommandInset href
LatexCommand href
name "Wikipedia"
target "https://en.wikipedia.org/wiki/MNIST_database"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Initial example
\end_layout

\begin_layout Standard
A typical neural network to solve the MNIST task might have
\end_layout

\begin_layout Itemize

\series bold
\emph on
784 inputs
\emph default
 
\series default
(pixels) connected to 
\series bold
\emph on
1,000 neurons
\series default
\emph default
, which are in turn connected to 
\series bold
\emph on
10 output targets 
\series default
\emph default
(one for each digit).
\end_layout

\begin_layout Itemize
Each layer is
\emph on
 fully connected
\emph default
 to the layer above (so each input pixel has a set of 
\series bold
\emph on
weights
\series default
\emph default
 that is connected to every neuron in the layer above).
\end_layout

\begin_layout Standard
This architecture can do quite well on handwritten digit recognition, but
 is orders of magnitude smaller than the human brain.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Images/Lecture1/Chapter2/typical-nn.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A graphical representation of the network described above, where 
\begin_inset Formula $x_{i}$
\end_inset

 are the inputs, 
\begin_inset Formula $h_{j}$
\end_inset

 are the hidden neurons and 
\begin_inset Formula $y_{k}$
\end_inset

 are the output class variables.
 The arrows represent the weights; the first layer connects 
\begin_inset Formula $784$
\end_inset

 pixels to 
\begin_inset Formula $1,000$
\end_inset

 hidden neurons (units) using 
\begin_inset Formula $784\times1,000=784,000$
\end_inset

 weights (typically represented as a 
\begin_inset Formula $784\times1,000$
\end_inset

 matrix).
 The next layer connects 
\begin_inset Formula $1,000$
\end_inset

 hidden units to 
\begin_inset Formula $10$
\end_inset

 output labels using 
\begin_inset Formula $10\times1,000=10,000$
\end_inset

 weights; thus this network has a total of 
\begin_inset Formula $10,000+784,000=794,000$
\end_inset

 weights.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Remark*
The number of neurons to use in the hidden layer is just a heuristic.
\end_layout

\begin_layout Subsection
A more detailed example
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://neuralnetworksanddeeplearning.com/chap1.html
\backslash
#a_simple_network_to_classify_handwritten_digits
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Remark*
\begin_inset Quotes eld
\end_inset

I have included this description here for consitency, but it includes more
 technical details that can be found in the later sections of the course.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Subsubsection
Setup
\end_layout

\begin_layout Standard
We can split the problem of recognising handwritten digits into two sub-problems
:
\end_layout

\begin_layout Enumerate
First, we would like a way of breaking an image containing many digits into
 a sequence of separate images, each containing a single digit.
 Humans solve this segmentation problem with ease, but it's challenging
 for a computer program to correctly break up the image.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Graphics
	filename ../Images/Lecture1/Chapter2/Step1.png
	lyxscale 20
	scale 20

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Once the image has been segmented, the program then needs to classify each
 individual digit.
\end_layout

\begin_layout Standard
We focus on the step 2 for the remaining of this subsection.
\end_layout

\begin_layout Standard
Consider using the following neural network
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Images/Lecture1/Chapter2/HandwritingClassificationNN.png
	lyxscale 30
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
The input layer
\series default
 of the network contains neurons encoding the values of the input pixels.
 As before, the input layer contains 
\begin_inset Formula $784=28\times28$
\end_inset

 neurons (for simplicity we have omitted most of the 
\begin_inset Formula $784$
\end_inset

 input neurons in the diagram).
 The input pixels are greyscale, with a value of 0.0 representing white,
 a value of 1.0 representing black, and in between values representing gradually
 darkening shades of grey.
\begin_inset Newline newline
\end_inset


\series bold
The second (or hidden) layer
\series default
 contains any number of neurons 
\begin_inset Formula $n$
\end_inset

, and we'll experiment with different values for 
\begin_inset Formula $n$
\end_inset

.
 The example shown illustrates a small hidden layer, containing just 
\begin_inset Formula $n=15$
\end_inset

 neurons.
\begin_inset Newline newline
\end_inset


\series bold
The output layer
\series default
 of the network contains 
\begin_inset Formula $10$
\end_inset

 neurons.
 We number the output neurons from 
\begin_inset Formula $0$
\end_inset

 through 
\begin_inset Formula $9$
\end_inset

, and figure out which neuron has the 
\emph on
highest activation value
\emph default
 (indicating that the input has been classified by the algorithm as this).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection

\series bold
Why use 10 output neurons?
\end_layout

\begin_layout Standard
A seemingly natural way of doing this is to use just 4 output neurons, treating
 each neuron as taking on a binary value, depending on whether the neuron's
 output is closer to 0 or to 1.
 
\end_layout

\begin_layout Standard

\series bold
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Plain Layout

\series bold
The ultimate justification is empirical: we can try out both network designs,
 and it turns out that, for this particular problem, the network with 10
 output neurons learns to recognise digits better than the network with
 4 output neurons.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To understand why we do this, it helps to think about what the neural network
 is doing from first principles:
\end_layout

\begin_layout Itemize
Consider first the case where we use 10 output neurons.
\end_layout

\begin_deeper
\begin_layout Itemize
The first output neuron is trying to decide whether or not the digit is
 a 0.
\end_layout

\begin_layout Itemize
It does this by weighing up evidence from the hidden layer of neurons.
\end_layout

\begin_layout Itemize
What are those hidden neurons doing? Well, just suppose for the sake of
 argument that the first neuron in the hidden layer detects whether or not
 an image like the following is present:
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename ../Images/Lecture1/Chapter2/mnist_top_left_feature.png
	lyxscale 20
	scale 20

\end_inset


\begin_inset Newline newline
\end_inset

It can do this by 
\emph on
heavily weighting input pixels which overlap with the image
\emph default
, and only lightly weighting the other inputs.
\end_layout

\begin_layout Itemize
In a similar way, suppose for the sake of argument that the second, third,
 and fourth neurons in the hidden layer detect whether or not the following
 images are present:
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename ../Images/Lecture1/Chapter2/mnist_other_features.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Itemize
These four images together make up the 0 image:
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename ../Images/Lecture1/Chapter2/mnist_complete_zero.png
	lyxscale 20
	scale 20

\end_inset


\begin_inset Newline newline
\end_inset

So if all four of these hidden neurons are firing then we can conclude that
 the digit is a 0.
\end_layout

\end_deeper
\begin_layout Note*
That is not the only sort of evidence we can use to conclude that the image
 was a 0 - we could legitimately get a 0 in many other ways (say, through
 
\emph on
translations 
\emph default
of the above images, or 
\emph on
slight distortions
\emph default
).
\end_layout

\begin_layout Remark*
Supposing the neural network functions in this way, if we had 4 outputs,
 then the first output neuron would be trying to decide what the most significan
t bit of the digit was and there is no easy way to relate that most significant
 bit to simple 
\emph on
shapes
\emph default
 like those shown above.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Images/Lecture1/Chapter2/HandwritingClassificationNN-binary.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
There is a way of determining the bitwise representation of a digit by adding
 an extra layer to the three-layer network above.
 The extra layer converts the output from the previous layer into a binary
 representation.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
