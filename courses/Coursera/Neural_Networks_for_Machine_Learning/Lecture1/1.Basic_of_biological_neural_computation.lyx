#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams-bytype
theorems-ams-extended-bytype
theorems-sec-bytype
eqs-within-sections
figs-within-sections
tabs-within-sections
\end_modules
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format pdf2
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 1cm
\headsep 1cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 3
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{section}{0}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Basics of biological neural computation
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Abstract
Artificial neural networks should be good at things that our brains are
 good at (i.e.
 parallel processing - e.g.
 vision), and bad at things our brains are bad at (e.g.
 long multiplication).
 In this chapter we look at the basic theory of how the human brain works,
 and the theory of neurons; in particular, we idealise neurons, removing
 the complicated details that are not essential for understanding the main
 principles.
\end_layout

\begin_layout Subsection
Synapses vs.
 RAM
\end_layout

\begin_layout Subsubsection
Disadvantages
\end_layout

\begin_layout Itemize
Synapses are 
\emph on
slow
\end_layout

\begin_layout Subsubsection
Advantages
\end_layout

\begin_layout Itemize
Synapses are very small and very low-power.
\end_layout

\begin_layout Itemize
Synapses adapt using locally available signals.
\end_layout

\begin_layout Subsection
How the brain works
\end_layout

\begin_layout Itemize
Each neuron receives inputs from other neurons
\end_layout

\begin_deeper
\begin_layout Itemize
A few neurons also connect to receptors.
\end_layout

\begin_layout Itemize
Cortical neurons use 
\begin_inset Quotes eld
\end_inset

spikes
\begin_inset Quotes erd
\end_inset

 to communicate.
\end_layout

\end_deeper
\begin_layout Itemize
The effect of each input line on the neuron is 
\series bold
\emph on
controlled by a synaptic weight
\series default
\emph default
 (weights can be positive or negative).
\end_layout

\begin_layout Itemize
The 
\series bold
\emph on
synaptic weights adapt
\series default
\emph default
 so the whole network learns to performs useful computations:
\end_layout

\begin_deeper
\begin_layout Itemize
Recognising objects.
\end_layout

\begin_layout Itemize
Understanding language.
\end_layout

\begin_layout Itemize
Making plans.
\end_layout

\begin_layout Itemize
Controlling the body.
\end_layout

\end_deeper
\begin_layout Itemize
Humans have about 
\begin_inset Formula $10^{11}$
\end_inset

 
\emph on
neurons
\emph default
, each with about 
\emph on

\begin_inset Formula $10^{4}$
\end_inset

 weights
\emph default
.
\end_layout

\begin_deeper
\begin_layout Itemize
A huge number of weights can affect the computations in a very short time.
\end_layout

\end_deeper
\begin_layout Subsection
Modularity and the brain
\end_layout

\begin_layout Itemize
Different bits of the cortex do different things (e.g.
 speech recognition).
\end_layout

\begin_deeper
\begin_layout Itemize
Specific task increase blood flow to specific regions.
\end_layout

\begin_layout Itemize
Local damage to the brain has specific effects; although the cortex looks
 pretty much the same all over (indeed, early brain damage makes functions
 relocate and in response to experience the brain had the ability to turn
 into special purpose hardware).
\end_layout

\end_deeper
\begin_layout Subsection
Simple models of neurons
\end_layout

\begin_layout Standard
To model things, it's beneficial to idealise them removing the complicated
 details that are not essential for understanding the main principles.
 Once we understand the basic principles, it's easy to add complexity to
 make the model more faithful.
 In the following types of neurons (aka.
 
\begin_inset Quotes eld
\end_inset

units
\begin_inset Quotes erd
\end_inset

), 
\begin_inset Formula $z:=b+\sum_{i}x_{i}w_{i}$
\end_inset


\end_layout

\begin_layout Subsubsection
Linear neurons
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y=z
\]

\end_inset


\end_layout

\begin_layout Standard
Simple, but computationally limited.
 The output 
\begin_inset Formula $y$
\end_inset

 is a function of the bias, 
\begin_inset Formula $b$
\end_inset

 of the neuron, plus the sum of all its incoming connections on the activity
 on an input neuron 
\begin_inset Formula $x_{i}$
\end_inset

 times the weight on that neuron, 
\begin_inset Formula $w_{i}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Binary threshold neurons (McCulloch-Pitts, 1943)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y=\mathbb{I}_{\left[z\geq0\right]}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Images/Lecture1/Chapter1/binary-neurons.png
	lyxscale 20
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
First compute a weighted sum of the inputs, then send out a fixed size spike
 of activity if the weighted sum exceeds a threshold.
 McCulloch and Pitts thought that each spike is like the truth values of
 a proposition, and each neuron combines truth values to compute the truth
 value of another proposition.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Rectified Linear Neuron (aka.
 linear threshold neurons)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y=\begin{cases}
z & \mbox{ if }z>0\\
0 & \mbox{ otherwise}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Images/Lecture1/Chapter1/rectified-linear-neurons.png
	lyxscale 20
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
They compute a linear weighted sum of their inputs, and the output is a
 
\emph on
non-linear
\emph default
 function of the total input.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Sigmoid Neurons
\end_layout

\begin_layout Standard
These are the 
\emph on
most commons neurons modelled by artificial neuron networks
\emph default
, introducing non-linearity in the model.
 
\begin_inset Formula 
\[
y=\frac{1}{1+e^{-z}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Images/Lecture1/Chapter1/sigmoid-neurons.png
	lyxscale 20
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Sigmoid neurons give a real-valued output that is a smooth and bounded function
 of their total input.
 Typically they use the logistic function, and have nice derivatives which
 make learning easy.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Stochastic Binary Neurons
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p:=\mathbb{P}\left(s=1\right)=\frac{1}{1+e^{-z}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Images/Lecture1/Chapter1/stochastic-binary-neurons.png
	lyxscale 20
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Stochastic Binary Neurons treat the output of the logistic equation as the
 probability of producing a spike in a short time window.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Remark*
Rectified linear units, the output is treated as the 
\emph on
Poisson rate
\emph default
 for spikes.
\end_layout

\end_body
\end_document
