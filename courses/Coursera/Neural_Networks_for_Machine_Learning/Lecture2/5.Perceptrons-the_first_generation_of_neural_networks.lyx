#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams-bytype
theorems-sec-bytype
theorems-ams-extended-bytype
eqs-within-sections
figs-within-sections
tabs-within-sections
\end_modules
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format pdf2
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 1cm
\headsep 1cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 3
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{section}{4}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Perceptrons - the first generation neural networks
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Abstract
Perceptrons were popularised by Frank Rosenblatt in the early 1960's, who
 appeared to have a 
\begin_inset Quotes eld
\end_inset

very powerful learning algorithm
\begin_inset Quotes erd
\end_inset

 (see section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:5.2.2"

\end_inset

) and grand claims were made for what they could learn to do.
 Unfortunately, his data was often bias.
 For example, he claimed that this algorithm could distinguish between tanks
 and trucks after processing many learning examples.
 What was not realised was that all the pictures of the trucks were taking
 on a cloudy day, whilst the pictures of tanks were taken on a clear sunny
 day, and all the perceptron was doing was measuring the total intensity
 of all the pixels.
 Likewise, Minsky and Papert in 1968 published a 
\begin_inset CommandInset href
LatexCommand href
name "book"
target "https://en.wikipedia.org/wiki/Perceptrons_%28book%29"

\end_inset

 called 
\begin_inset Quotes eld
\end_inset

Perceptrons
\begin_inset Quotes erd
\end_inset

 that analysed that they could do and show their limitations.
 Many people thought these limitations applied to all neural networks (even
 though Minsky and Papert knew the book did not answer this).
\end_layout

\begin_layout Abstract
In this chapter we describe the Perceptron, how learning proceeds, and the
 limitations of using perceptrons.
\end_layout

\begin_layout Abstract
(Useful resource 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://neuralnetworksanddeeplearning.com/chap1.html#perceptrons
\end_layout

\end_inset

.)
\end_layout

\begin_layout Subsection
The standard paradigm for statistical pattern recognition
\end_layout

\begin_layout Definition
A 
\series bold
perceptron
\series default
 is an algorithm for learning a binary classifier.
\end_layout

\begin_layout Remark*
A useful way you can think about the perceptron is that it is a device that
 makes decisions by weighing up evidence - the more weight a particular
 input has indicates that that input is more important (w.r.t.
 the other inputs).
\end_layout

\begin_layout Note*
There are actually many different kinds of perceptrons; the standard kind,
 typically called an 
\series bold
alpha perceptron
\series default
 (see figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:alpha-perceptron"

\end_inset

), consists of some inputs which are then converted into feature activities.
\end_layout

\begin_layout Standard
The standard paradigm for statistical pattern recognition involves:
\end_layout

\begin_layout Enumerate
Convert the raw input vector into a vector of feature activations.
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Quotes eld
\end_inset

We use handwritten programs based on commonsense to define the features
 (so this part of the system does not learn).
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate

\emph on
Learn
\emph default
 how to weight each of the feature activations to get a single scalar quantity.
\end_layout

\begin_layout Enumerate
If this quantity is above some threshold, decide that the input vector is
 a positive example of the target class.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Images/Lecture2/Chapter5/standard-perceptron-architecture.png
	lyxscale 20
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The standard perceptron architecture; a perceptron is a particular example
 of a statistical pattern recognition system.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Perceptron learning
\end_layout

\begin_layout Subsubsection
Using the same rule for learning the weights as that for the biases
\end_layout

\begin_layout Standard
The perceptron uses binary threshold neurons (recall 
\begin_inset CommandInset href
LatexCommand href
name "chapter 1"
target "http://andyandy1992.github.io/MyMOOCs/courses/Coursera/Neural_Networks_for_Machine_Learning/Lecture1/1.Basic_of_biological_neural_computation.pdf"

\end_inset

) as its decision units.
 A threshold is equivalent to having a negative bias; that is, 
\begin_inset Formula $y=\mathbb{I}_{\left[\sum_{i}x_{i}w_{i}\geq\theta\right]}\iff y=\mathbb{I}_{\left[z\geq0\right]}$
\end_inset

 where 
\begin_inset Formula $b=-\theta(=w_{0})$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Images/Lecture2/Chapter5/biases-as-weights.png
	lyxscale 20
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
\begin_inset CommandInset label
LatexCommand label
name "fig:alpha-perceptron"

\end_inset

Biases as weights
\series default
: We can avoid having to figure out a separate learning rule for the bias
 by using this trick - a bias is 
\emph on
exactly equivalent 
\emph default
to a weight on an extra input line that always has an activity of 1.
 We can now learn a bias as if it were a weight.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Remark*
Biologically, the bias is a measure of how easy it is to get the perceptron
 to 
\emph on
fire
\emph default
.
\end_layout

\begin_layout Subsubsection
\begin_inset CommandInset label
LatexCommand label
name "sub:5.2.2"

\end_inset

The perceptron convergence procedure: Training binary output neurons as
 classifiers
\end_layout

\begin_layout Standard
To train binary output neurons as classifiers, we can use the following
 steps:
\end_layout

\begin_layout Enumerate

\series bold
Deal with threshold: 
\series default
Add an extra component with value 1 to each input vector (i.e.
 
\begin_inset Formula $\mathbf{x}=\left(1,\mathbf{x}\right)$
\end_inset

).
 The bias weight on this component is minus the threshold (so we can 
\begin_inset Quotes eld
\end_inset

forget
\begin_inset Quotes erd
\end_inset

 the threshold, i.e.
 
\begin_inset Formula $\mathbf{w}=\left(b,\mathbf{w}\right)$
\end_inset

).
\end_layout

\begin_layout Enumerate
Pick training cases using any policy that ensures that every training case
 will keep getting picked.
 Update the weights as follows:
\end_layout

\begin_deeper
\begin_layout Enumerate
If the output unit is correct, leave its weight alone.
\end_layout

\begin_layout Enumerate
If the output unit is incorrectly outputs a 0, add the input vector to the
 weight vector.
\end_layout

\begin_layout Enumerate
If the output unit is incorrectly outputs a 1, subtract the input vector
 from the weight vector.
\end_layout

\end_deeper
\begin_layout Remark*
The set of candidate hypotheses considered in perceptron learning is the
 set of all possible real-valued weight vectors 
\begin_inset Formula $H=\left\{ \mathbf{w}:\mathbf{w}\in\mathbb{R}^{n}\right\} $
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Note*
This is 
\series bold
\emph on
\bar under
guaranteed
\series default
\emph default
\bar default
 to find a set of weights that gets the right answer for every training
 case...
\color red
IF ANY SUCH SET EXISTS! 
\color inherit
Unfortunately for many interesting problems, 
\emph on
\bar under
no such set of weights exist
\emph default
\bar default
; this depends very much on what features you use.
 So for many problems, the difficult bit is deciding what features to use.
\end_layout

\end_deeper
\begin_layout Subsection
A geometrical view of perceptrons learning
\end_layout

\begin_layout Standard
We are going to start thinking about hyperplanes in high-demensional spaces;
 this is difficult.
 To deal with say, hyperplanes in a 14-dimensional space, visualise a 3-D
 space and say 
\begin_inset Quotes eld
\end_inset

14
\begin_inset Quotes erd
\end_inset

 to yourself very loudly (everyone does it).
\end_layout

\begin_layout Note*
Going from 13-D to 14-D creates as much extra complexity as going from 2-D
 to 3-D.
\end_layout

\begin_layout Subsubsection
Weight space
\end_layout

\begin_layout Itemize
The weight-space has 1-D per weight; a point in the space represents a particula
r setting of all the weights.
 
\end_layout

\begin_layout Itemize
Assuming that we have eliminated the threshold, each 
\emph on
training case
\emph default
 can be represented as a 
\emph on
hyperplane
\emph default
 through the origin.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Images/Lecture2/Chapter5/weight-space-1.png
	lyxscale 20
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
An input vector with correct answer=1:
\series default
 Each training case defines a plane (black line) - the plane goes through
 the origin and is perpendicular to the 
\color blue
input vector (blue arrow)
\color inherit
.
 For each training case, the weights must lie on 
\emph on
one
\emph default
 side of this hyperplane to get the 
\emph on
answer correct
\emph default
; on one side of the plane the output is wrong because the scalar product
 of the weight vector with the input vector,
\emph on
 
\emph default

\begin_inset Formula $z=\sum_{i}x_{i}w_{i}=\left\Vert \mathbf{x}\right\Vert \left\Vert \mathbf{w}\right\Vert \cos\theta$
\end_inset

, has the wrong sign (it should be positive so that 
\begin_inset Formula $z\geq0\Rightarrow y=\mathbb{I}_{\left[z\geq0\right]}=1$
\end_inset

, so that the angle should be less than 
\begin_inset Formula $90^{\circ}$
\end_inset

).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Remark*
We can think of the inputs as partitioning the space into two halves - weights
 lying in one half will get the answer correct while on the other half they
 will give the incorrect answer (which half is determined by the output
 class).
 That is, the inputs will 
\emph on
constrain 
\emph default
the set of weights that give the correct classification results.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Images/Lecture2/Chapter5/weight-space-0.png
	lyxscale 20
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
An input vector with correct answer=0:
\series default
 This time for each training case, the weights must lie on 
\emph on

\begin_inset Quotes eld
\end_inset

other
\begin_inset Quotes erd
\end_inset


\emph default
 side of the hyperplane to get the 
\emph on
answer correct
\emph default
, because we want the 
\emph on
scalar product
\emph default
 of the weight vector with the input vector,
\emph on
 
\emph default

\begin_inset Formula $z=\sum_{i}x_{i}w_{i}=\left\Vert \mathbf{x}\right\Vert \left\Vert \mathbf{w}\right\Vert \cos\theta$
\end_inset

 to be less than 0 (so 
\begin_inset Formula $y=\mathbb{I}_{\left[z\geq0\right]}=0$
\end_inset

); that is the angle should be greater than 
\begin_inset Formula $90^{\circ}$
\end_inset

, so the sign is negative.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
If there are any weight vectors that get the right answer for all cases,
 they lie in a hyper-cone with its apex at the origin.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Images/Lecture2/Chapter5/weight-space-cone.png
	lyxscale 20
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
The Cone of Feasible Solutions:
\series default
 To get all training cases right, we need to find a point on the right side
 of all the planes - 
\emph on
inside the cone of feasible solution
\emph default
.
 Of course there 
\emph on
may not exist
\emph default
 such a cone - it may be that there are no weight vectors that get the right
 answer for all the training cases (but if there does exist such, it'll
 be inside the cone).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Remark*

\series bold
The problem is convex: 
\series default
If we find 2 good weight vectors (that lie in the cone for all training
 cases), the average of them is also a good weight vector.
 In general machine learning problems, if you can find a 
\emph on
convex 
\emph default
problem it makes life a lot easier.
\end_layout

\begin_layout Subsection
Why the learning procedure works
\end_layout

\begin_layout Definition

\series bold
Generously feasible
\series default
 
\series bold
weight vectors
\series default
 are weight vectors that lie within the feasible region by a margin at least
 as great as the length of the input vector that defines each constraint
 plane.
\end_layout

\begin_deeper
\begin_layout Definition
The 
\series bold
squared distance
\series default
 of a weight vector is the squared sum of all orthogonal vectors.
\end_layout

\begin_layout Definition
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Neural_Networks_for_Machine_Learning/Images/Lecture2/Chapter5/squared-distanced.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The 
\series bold
squared distance
\series default
 of a (feasible) weight vector.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm-5.1"

\end_inset

Every time the perceptron makes a mistake, the squared distance to all of
 generously feasible weight vectors is always decreased by at least the
 squared length of the input (or update) vector.
\end_layout

\begin_layout Proof

\emph on
(Sketch)
\end_layout

\begin_layout Proof
Each time the perceptron makes a mistake, the current weight vector moves
 to decrease its squared distance from every generously feasible weight
 vector by at least the squared length of the current input vector.
\end_layout

\begin_layout Proof
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Neural_Networks_for_Machine_Learning/Images/Lecture2/Chapter5/why-learning-procedure-works.png
	lyxscale 20
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Note in this case, the gold point is not generously feasible (as it is not
 within the feasible region by a margin at least as great as the length
 of the input vector), so we omit it from the squared distance caluculation.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Proof
Assuming none of the input vectors are infinitesimally small, after a finite
 number of mistakes the weight vector must lie in the feasible region (IF
 THIS REGION EXISTS), which would stop it making mistakes.
\end_layout

\begin_layout Subsection
What perceptron can't do
\end_layout

\begin_layout Itemize
If you are allowed to choose the features by hand and if you use enough
 features, you can make a perceptron do almost almost anything.
\end_layout

\begin_layout Itemize
But once the hand-coded features have been determined, there are very strong
 limitations on what a perceptron can learn.
\end_layout

\begin_layout Example

\series bold
\emph on
XOR
\end_layout

\begin_layout Example
Consider the task of establishing if two single bit features are the same:
\end_layout

\begin_layout Example
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="2">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Positive Cases (same)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Negative Cases (different)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left(1,1\right)\Rightarrow1$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left(0,1\right)\Rightarrow0$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left(0,0\right)\Rightarrow1$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\left(1,0\right)\Rightarrow0$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The 4 input-output pairs give 4 inequalities that are impossible to satisfy:
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula 
\[
\left.\begin{array}{c}
1\cdot w_{1}+1\cdot w_{2}\geq\theta\\
0\cdot w_{1}+0\cdot w_{2}\geq\theta\\
0\cdot w_{1}+1\cdot w_{2}<\theta\\
1\cdot w_{1}+0\cdot w_{2}<\theta
\end{array}\right\} \Rightarrow\begin{array}{c}
w_{1}+w_{2}\geq\theta\\
0\geq\theta\\
w_{2}<\theta\\
w_{1}<\theta
\end{array}
\]

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Example
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Images/Lecture2/Chapter5/no-hyperplane.png
	lyxscale 20
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
Boolean function AND
\series default
: This set of training examples is not 
\series bold
linearly separable 
\series default
(the green points cannot be separated from the red points by a straight
 line) - thus the Boolean function AND can also not be represented by perceptron
s.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Remark*
A perceptron can implement a NAND gate (see the useful reference in the
 abstract), and the NAND gate is universal for computation (that is, we
 can build any computation up out of NAND gates).
 Since NAND gates are universal for computation, it follows that perceptrons
 are also universal for computation.
\end_layout

\begin_layout Itemize
Minsky and Papert proved that the above learning procedure converges within
 a finite number of iterations, 
\emph on
provided the training examples are linearly separable
\emph default
.
 If the data are not linearly separable, convergence is not assured.
\end_layout

\begin_deeper
\begin_layout Itemize
Minsky and Papert's 
\begin_inset Quotes eld
\end_inset

Group Invariance Theorem
\begin_inset Quotes erd
\end_inset

 says that the part of a perceptron that learns cannot learn to recognise
 patterns that have undergone transformations that form a group (such has
 translations with 
\series bold
wrap-around
\series default
).
\end_layout

\end_deeper
\begin_layout Itemize
To deal with such transformations, a perceptron needs to use multiple feature
 units (
\emph on
so the tricky part of pattern recognition must be solved by the hand-coded
 feature detectors, not the learning procedure
\emph default
).
\end_layout

\begin_layout Remark*
This illustrates that for neural networks to be powerful, we need them to
 be able to learn the feature detectors (it's not enough to just learn the
 weights of feature detectors) - hence second generation neural networks.
\end_layout

\end_body
\end_document
