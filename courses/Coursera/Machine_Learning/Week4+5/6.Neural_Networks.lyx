#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams-bytype
theorems-ams-extended-bytype
theorems-sec-bytype
eqs-within-sections
figs-within-sections
tabs-within-sections
\end_modules
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format pdf2
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered true
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "urlcolor=blue"
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 2
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\boxbgcolor #ffff7f
\branch How many Nobel Prizes?
\selected 1
\filename_suffix 0
\color #faf0e6
\end_branch
\branch Guess is 150
\selected 0
\filename_suffix 0
\color #faf0e6
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 1cm
\headsep 1cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 3
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{section}{5}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Neural Networks
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Abstract
This chapter introduces how we can dealing with non-linear hypotheses with
 neural network.
 Although because I have completed the dedicated course 
\begin_inset Quotes eld
\end_inset

Neural networks for machine learning
\begin_inset Quotes erd
\end_inset

 on Coursera, please see this course for more explicit details (to avoid
 duplication).
 This chapter predominately discusses neural networks for classification.
\end_layout

\begin_layout Subsection
Key Remarks 
\end_layout

\begin_layout Enumerate
Neural networks are one of a few ML algorithms where we continue to improve
 our prediction accuracy as we increase the number of training examples
 (most others tend to level off even if we increase the data by a significant
 amount).
\end_layout

\begin_layout Enumerate
Build models that use hardware as efficiently as possible (that way when
 hardware improves your learning speeds up essentially for free).
\end_layout

\begin_layout Enumerate
Set a baseline as the fastest your code can ever run, i.e.
 the speed of light, or in speech recognition it's 0% WER (improving speed
 by 10x on already badly written code is not an effective way to analyse
 this).
\end_layout

\begin_layout Enumerate
As discussed at 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://youtu.be/MVyauNNinC0?t=36m18s
\end_layout

\end_inset

, one of the best ways to train a DNN is to try to fool/trick them.
\end_layout

\begin_layout Subsection
Representation
\end_layout

\begin_layout Notation

\series bold
Activation units and vector representation of neural networks
\end_layout

\begin_layout Notation
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../Images/Week4+5/Chapter6/nn-notation.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A network with 
\begin_inset Formula $s_{j}$
\end_inset

 units in layer 
\begin_inset Formula $j$
\end_inset

, 
\begin_inset Formula $s_{j+1}$
\end_inset

 units in layer 
\begin_inset Formula $j+1$
\end_inset

, then 
\begin_inset Formula $\Theta^{\left(j\right)}$
\end_inset

 will be of dimension 
\begin_inset Formula $s_{j+1}\times\left(s_{j}+1\right)$
\end_inset

.
 The process of 
\series bold
\emph on
computing
\emph default
 
\begin_inset Formula $h_{\Theta}\left(x\right)$
\end_inset

 
\series default
is called
\series bold
 forward propagation
\series default
 (since we must start with the activations of the input units, and then
 we forward propagate these to compute the activations in the hidden layer
 and so on).
 
\emph on
(c.f back propagation, where we are given 
\begin_inset Formula $h_{\Theta}\left(x\right)$
\end_inset

 and want to learn the weights/parameters.)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Note*
There is a 
\begin_inset Formula $\Theta$
\end_inset

 matrix for each layer 
\begin_inset Formula $l$
\end_inset

 in the network, with 
\begin_inset Formula $\Theta_{a_{l},a_{l-1}}^{(l-1)}$
\end_inset

 (
\begin_inset Formula $a_{l}:=$
\end_inset

index of activation node in layer 
\begin_inset Formula $l$
\end_inset

 being computer, 
\begin_inset Formula $a_{l-1}:=$
\end_inset

index of activation node in layer 
\begin_inset Formula $l-1$
\end_inset

 being multiplied by).
\end_layout

\begin_layout Remark*
Looking at how we compute each activation unit, 
\begin_inset Formula $a_{i}^{\left(j\right)}$
\end_inset

, we notice that what this neural network is just doing logistic regression
 (with 
\series bold

\begin_inset Quotes eld
\end_inset

learned
\begin_inset Quotes erd
\end_inset

 inputs
\series default
).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../Images/Week4+5/Chapter6/nn-adding_complexity.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Neural network are used to learn more complex functions with each layer.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Multiple output units: One-vs-all
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../Images/Week4+5/Chapter6/nn-one_vs_all.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
In the one vs.
 all (equal to 4 here) representation, instead of letting 
\begin_inset Formula $y\in\left\{ 0,1\right\} $
\end_inset

, we let 
\begin_inset Formula $y\in\left\{ \left[1,0,0,0\right]^{T},\left[0,1,0,0\right]^{T},\left[0,0,1,0\right]^{T},\left[0,0,0,1\right]^{T}\right\} $
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Evaluation: Cost Function
\end_layout

\begin_layout Notation
\begin_inset ERT
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Formula 
\begin{eqnarray*}
L & := & \mbox{total \# layers in net work}\\
s_{l} & := & \mbox{\# units (excluding bias unit) in layer }l\\
s_{L} & := & \mbox{\# units in output layer}\\
K & := & \#\mbox{ output units/classes}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The cost function for neural networks is a generalisation of the cost function
 used from logistic regression (with regularisation); so 
\begin_inset Formula $h_{\Theta}\left(x\right)\in\mathbb{R}^{K}$
\end_inset

 (i.e.
 
\begin_inset Formula $\left(h_{\Theta}\left(x\right)\right)_{i}=$
\end_inset

probability that the 
\begin_inset Formula $i^{th}$
\end_inset

 output is 
\begin_inset Formula $1$
\end_inset

), and we take the sum over these 
\begin_inset Formula $K$
\end_inset

 output units in the cost function:
\begin_inset Formula 
\[
J\left(\Theta\right)=-\frac{1}{m}\left[\sum_{i=1}^{m}\sum_{k=1}^{K}y_{k}^{\left(i\right)}\log\left(h_{\Theta}\left(x^{\left(i\right)}\right)\right)_{k}+\left(1-y_{k}^{\left(i\right)}\right)\log\left(1-\left(h_{\Theta}\left(x^{\left(i\right)}\right)\right)_{k}\right)\right]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^{s_{l+1}}\left(\Theta_{ji}^{\left(l\right)}\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Remark*
The first half of the cost function is saying (
\begin_inset Quotes eld
\end_inset

average sum of logistic regression
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_deeper
\begin_layout Itemize
For each training data example (
\begin_inset Formula $1,...,m$
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Itemize
Some for each position in the output vector.
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsection
Optimisation: Stochastic gradient descent using backpropagation to compute
 the gradient.
\end_layout

\begin_layout Standard
To find 
\begin_inset Formula $\min_{\Theta}J\left(\Theta\right)$
\end_inset

 using gradient descent or one of the advanced optimisation algorithms,
 we need to compute 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

 and 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{ij}^{\left(l\right)}}J\left(\Theta\right)$
\end_inset

 (recall 
\begin_inset Formula $\Theta_{ij}^{\left(l\right)}\in\mathbb{R}$
\end_inset

).
\end_layout

\begin_layout Subsubsection
Gradient Computation for the case of one training example
\end_layout

\begin_layout Standard
Computing the gradient in the case of one training example 
\begin_inset Formula $\left(\mathbf{x},\mathbf{y}\right)$
\end_inset

, we see that:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../Images/Week4+5/Chapter6/gradient-computation-1-training-example.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
Step 1 - Forward Propagation
\series default
 
\series bold
(Calculating the activations 
\begin_inset Formula $\mathbf{a}^{\left(i\right)}$
\end_inset

)
\series default

\begin_inset Newline newline
\end_inset

This algorithm takes the initial input into that network and pushes the
 input through the network to the generation of an output hypothesis.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../Images/Week4+5/Chapter6/backpropagation-1-training-example.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
Step 2 - Backpropagation (
\series default
Calculating the gradients 
\begin_inset Formula $\boldsymbol{\mathbf{\delta}}^{\left(i\right)}$
\end_inset

)
\begin_inset Newline newline
\end_inset

This algorithm takes the output you got from your network, compares it to
 the real value 
\series bold

\begin_inset Formula $y$
\end_inset


\series default
 and calculates how wrong the network was (i.e.
 how wrong the parameters were) It then, using the error you've just calculated,
 back-calculates the error associated with each unit from the preceding
 layer (i.e.
 layer 
\begin_inset Formula $L-1$
\end_inset

).
 These "error" measurements for each unit can be used to calculate the partial
 derivatives, which can then be used to calculate the partial derivative
 for gradient descent.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Note*
The 
\begin_inset Formula $.*$
\end_inset

 denotes element-wise multiplication.
\end_layout

\begin_layout Remark*
We do not calculate 
\begin_inset Formula $\boldsymbol{\delta}^{\left(1\right)}$
\end_inset

 as the first layer does not have any error associated with it (it's just
 the features we observed in our training set).
\end_layout

\begin_deeper
\begin_layout Remark*
When the cost function is 
\series bold
not 
\series default
regularised (i.e.
 
\begin_inset Formula $\lambda=0$
\end_inset

), it can be shown that 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{ij}^{\left(l\right)}}J\left(\Theta\right)=a_{j}^{\left(l\right)}\delta_{i}^{\left(l+1\right)}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Subsubsection
Gradient Computation for 
\begin_inset Formula $m$
\end_inset

 training examples
\end_layout

\begin_layout Standard
For a training set 
\begin_inset Formula $\left\{ \left(x^{\left(1\right)},y^{\left(1\right)}\right),...,\left(x^{\left(m\right)},y^{\left(m\right)}\right)\right\} $
\end_inset

:
\end_layout

\begin_layout Standard

\series bold
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Algorithm

\series bold
Backpropagation Algorithm
\end_layout

\begin_layout Algorithm
Set 
\begin_inset Formula $\Delta_{ij}^{\left(l\right)}=0$
\end_inset

, 
\begin_inset Formula $\forall l,i,j$
\end_inset

 
\shape up
(i.e.
 this has each node as one dimension and each training data example as the
 other)
\shape default
.
\end_layout

\begin_layout Algorithm
For 
\begin_inset Formula $i=1,...,m$
\end_inset

:
\end_layout

\begin_layout Algorithm
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

Set 
\begin_inset Formula $a^{\left(1\right)}=x^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Algorithm
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

Perform forward propagation to compute 
\begin_inset Formula $a^{\left(l\right)}$
\end_inset

 for 
\begin_inset Formula $l=2,3,...,L$
\end_inset


\end_layout

\begin_layout Algorithm
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

Using 
\begin_inset Formula $y^{\left(i\right)}$
\end_inset

, compute 
\begin_inset Formula $\delta^{\left(L\right)}=a^{\left(L\right)}-y^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Algorithm
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

Compute 
\begin_inset Formula $\delta^{\left(L-1\right)}$
\end_inset

,
\begin_inset Formula $\delta^{\left(L-2\right)}$
\end_inset

,...,
\begin_inset Formula $\delta^{\left(2\right)}$
\end_inset


\end_layout

\begin_layout Algorithm
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Formula $\Delta_{ij}^{\left(l\right)}:=\Delta_{ij}^{\left(l\right)}+a_{j}^{\left(l\right)}\delta_{i}^{\left(l+1\right)}$
\end_inset

 (or vectorised form 
\begin_inset Formula $\boldsymbol{\Delta}^{\left(l\right)}:=\boldsymbol{\Delta}^{\left(l\right)}+\delta^{\left(l+1\right)}\left(a^{\left(l\right)}\right)^{T}$
\end_inset

)
\end_layout

\begin_layout Algorithm
\begin_inset Formula $D_{ij}^{\left(l\right)}:=\begin{cases}
\frac{1}{m}\Delta_{ij}^{\left(l\right)}+\lambda\Theta_{ij}^{\left(l\right)} & \mbox{ if }j\neq0\\
\frac{1}{m}\Delta_{ij}^{\left(l\right)} & \mbox{ if }j=0
\end{cases}$
\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Remark*
For quite a complicate proof, it can be shown that the partial derivatives
 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{ij}^{\left(l\right)}}J\left(\Theta\right)=D_{ij}^{\left(l\right)}$
\end_inset

.
\end_layout

\begin_layout Subsection
Backpropagation in practice
\end_layout

\begin_layout Subsubsection
Unrolling parameters
\end_layout

\begin_layout Standard
With neural networks, we are working with sets of matrices:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Theta_{1},\Theta_{2},\Theta_{3},...;D_{1},D_{2},D_{3},...
\]

\end_inset


\end_layout

\begin_layout Standard
In order to use optimizing functions such as 
\family typewriter
fminunc()
\family default
, we need to "unroll" all the elements and put them into one long vector
 (MATLAB):
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

thetaVector = [ Theta1(:); Theta2(:); Theta3(:) ];
\end_layout

\begin_layout Plain Layout

deltaVector = [ D1(:); D2(:); D3(:) ];
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Gradient checking
\end_layout

\begin_layout Standard
Backpropagation has a lot of details.
 Small bugs can easily creep in so that it looks like 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

 is decreasing, but in reality it may not be decreasing by as much as it
 should.
\end_layout

\begin_layout Standard
We can approximate the derivative of our cost function, for some small 
\begin_inset Formula $\varepsilon>0$
\end_inset

, as
\begin_inset Formula 
\[
\frac{\partial}{\partial\Theta}J\left(\Theta\right)\approx\frac{J\left(\Theta+\varepsilon\right)-J\left(\Theta-\varepsilon\right)}{2\varepsilon}
\]

\end_inset

and for multiple theta matrices, we can approximate the derivative w.r.t.
 
\begin_inset Formula $\Theta_{j}$
\end_inset

 as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial}{\partial\Theta_{j}}J\left(\Theta\right)\approx\frac{J\left(\Theta_{1},...\Theta_{j-1},\Theta_{j}+\varepsilon,\Theta_{j+1},...,\Theta_{n}\right)-J\left(\Theta_{1},...\Theta_{j-1},\Theta_{j}-\varepsilon,\Theta_{j+1},...,\Theta_{n}\right)}{2\varepsilon}
\]

\end_inset


\end_layout

\begin_layout Remark*
Andrew Ng typically uses 
\begin_inset Formula $\varepsilon=10^{-4}$
\end_inset

.
\end_layout

\begin_layout Standard
Finally check that gradApprox 
\begin_inset Formula $\approx$
\end_inset

 deltaVector.
\end_layout

\begin_layout Note*
Once you've verified once that your backpropagation algorithm is correct,
 then you don't need to compute gradApprox again.
 The code to compute gradApprox is very slow.
\end_layout

\begin_layout Subsubsection
Random initialisation
\end_layout

\begin_layout Standard
Initializing all theta weights to 
\begin_inset Formula $0$
\end_inset

 does not work with neural networks.
 When we backpropagate, all nodes will update to the same value repeatedly.
 Thus we use 
\series bold
random initialization
\series default
, which serves the purpose of symmetry breaking.
\end_layout

\begin_layout Standard
Initialise each 
\begin_inset Formula $\Theta^{\left(l\right)}\in\mathbb{R}^{n_{l-1},n_{l}}$
\end_inset

 (where 
\begin_inset Formula $n_{l}:=$
\end_inset

# neurons in layer 
\begin_inset Formula $l$
\end_inset

 (I WORKED THIS OUT LATE - CHECK)) to a random value between 
\begin_inset Formula $\left[-\varepsilon,\varepsilon\right]$
\end_inset

 with
\begin_inset Formula 
\[
\varepsilon=\frac{\sqrt{6}}{\sqrt{n_{l}+n_{l-1}}}
\]

\end_inset


\end_layout

\begin_layout Standard
E.g.
 
\begin_inset Formula $\Theta^{\left(1\right)}=\mathtt{rand}\left(n_{l-1},n_{l}\right)*2\varepsilon-\varepsilon\in\mathbb{R}^{n_{l-1},n_{l}}$
\end_inset

 (where the 
\family typewriter
rand
\family default
 function produces a 
\begin_inset Formula $n_{l-1}\times n_{l}$
\end_inset

 matrix).
\end_layout

\begin_layout Subsection
Implementation summary and some best practices of using neural networks
 for machine learning
\end_layout

\begin_layout Enumerate

\series bold
\bar under
REPRESENTATION:
\series default
\bar default
 Pick a network architecture, including how many hidden units there are
 in each layer and how many layers in total.
\end_layout

\begin_deeper
\begin_layout Enumerate
The higher the 
\begin_inset Formula $\#\mbox{ hidden units per layer}$
\end_inset

is usually produces more accurate results, but we must take into account
 the computation cost.
\end_layout

\begin_layout Enumerate

\emph on
\begin_inset Formula $\#\mbox{ hidden layers}:$
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate

\emph on
\begin_inset Quotes eld
\end_inset

1 hidden layer is sufficient for the large majority of problems
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Enumerate
If the data
\series bold
 linearly separable
\series default
, then you 
\series bold
don't need any hidden 
\series default
layers at all.
\end_layout

\begin_layout Enumerate
DNN
\begin_inset Formula $=$
\end_inset

more than 3; most papers on deep learning use networks with 5-7 layers.
\end_layout

\begin_layout Enumerate
Geoff Hinton: "Add layers until you overfit (if you’re not overfitting,
 your network isn’t big enough.).
 Then add dropout or another regularization method."
\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset Formula $\#\mbox{ neurons for each layer:}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
Roughly same for all hidden layers and usually at 1 to 3 times the input
 variables (or mean of the # input and # output nodes).
\end_layout

\begin_layout Enumerate
Yoshia Bengio: "It is a hyper-parameter to be optimized, as usual."
\end_layout

\begin_layout Enumerate
WARNINGS:
\end_layout

\begin_deeper
\begin_layout Enumerate

\series bold
Too few nodes
\series default
 will lead to high error for your system as the predictive factors might
 be too complex for a small number of nodes to capture.
 
\end_layout

\begin_layout Enumerate

\series bold
Too many nodes
\series default
 will overfit to your training data and not generalise well.
\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
\begin_inset Formula $\#\mbox{ input units}=\mbox{dimension of features }x^{\left(i\right)}$
\end_inset

; 
\begin_inset Formula $\#\mbox{ output units}=\#\mbox{ classes}$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
\bar under
EVALUATION:
\series default
\bar default
 Randomly initialise the weights (as discussed above).
\end_layout

\begin_layout Enumerate

\series bold
\bar under
EVALUATION:
\series default
\bar default
 Implement forward propagation to get 
\begin_inset Formula $h_{\theta}\left(x^{\left(i\right)}\right)$
\end_inset

.
\end_layout

\begin_layout Enumerate

\series bold
\bar under
EVALUATION:
\series default
\bar default
 Compute the cost function
\end_layout

\begin_layout Enumerate

\series bold
\bar under
EVALUATION:
\series default
\bar default
 Implement back propagation to compute partial derivatives.
\end_layout

\begin_deeper
\begin_layout Enumerate
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week4+5/Chapter6/backprop-summary.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Backpropagation is usually done with a for loop over training examples,
 like above.
 It can be done without a for loop, but this is much more complicated way
 of doing things.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Use gradient checking to confirm your back propagation works.
 Then disable gradient checking.
\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
\bar under
OPTIMISATION:
\series default
\bar default
 Use gradient descent or a built-in optimization function to minimize the
 cost function with the weights in theta.
\end_layout

\begin_deeper
\begin_layout Enumerate
If 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

 is not convex, then some algorithms can be susceptible to local minimum.
 In practice this is not usually a huge problem, but we can't guarantee
 programs with find global optimum should find good local optimum at least.
\end_layout

\end_deeper
\begin_layout Subsubsection
Dealing with overfitting
\end_layout

\begin_layout Itemize
The easiest and most common method to reduce overfitting on image data is
 to artificially enlarge the dataset using label-preserving transformations
 (e.g.
 by flipping the image, rotating it slightly, removing some of the image).
\end_layout

\end_body
\end_document
