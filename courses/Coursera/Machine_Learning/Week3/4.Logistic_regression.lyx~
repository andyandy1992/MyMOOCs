#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams-bytype
theorems-ams-extended-bytype
theorems-sec-bytype
eqs-within-sections
figs-within-sections
tabs-within-sections
\end_modules
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format pdf2
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered true
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "urlcolor=blue"
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 2
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\boxbgcolor #ffff7f
\branch How many Nobel Prizes?
\selected 1
\filename_suffix 0
\color #faf0e6
\end_branch
\branch Guess is 150
\selected 0
\filename_suffix 0
\color #faf0e6
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 1cm
\headsep 1cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 3
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{section}{3}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Logistic regression
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Abstract
We now switch from regression problems to classification problems.
 Don't be confused by the name "Logistic Regression"; it is named that way
 for historical reasons and is actually an approach to classification problems,
 not regression problems.
 We begin by studying 
\series bold
Binary Classification Problem
\series default
 where a machine learning algorithm classifies the inputs into one of two
 possible the outputs, and 
\series bold
Multiclass Classification 
\series default
where there are many possible classes (this is just an extension of binary
 classification).
\end_layout

\begin_deeper
\begin_layout Subsection
Binary Classification Problem: Representation
\end_layout

\end_deeper
\begin_layout Standard
Now our output vector 
\begin_inset Formula $y\in\left\{ 0,1\right\} $
\end_inset

 where 
\begin_inset Formula $0$
\end_inset

 typically represents the 
\begin_inset Quotes eld
\end_inset

negative
\begin_inset Quotes erd
\end_inset

 class and 
\begin_inset Formula $1$
\end_inset

 as the 
\begin_inset Quotes eld
\end_inset

positive class
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Remark*
One method is to use linear regression:
\end_layout

\begin_layout Remark*
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week3/Chapter4/Classifying_cancer.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Linear regression can be used for classification, e.g.
 in this example by maping all predictions 
\begin_inset Formula $h_{x}\left(\boldsymbol{\theta}\right)\geq0.5$
\end_inset

 to 
\begin_inset Formula $1$
\end_inset

 and 
\begin_inset Formula $h_{x}\left(\boldsymbol{\theta}\right)<0.5$
\end_inset

 to 
\begin_inset Formula $0$
\end_inset

.
 This method doesn't work well because 
\emph on
classification is not actually a linear function
\emph default
 - what if we had a single 
\series bold
Yes 
\series default
with a very small tumour?
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Hypothesis Representation
\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $y\in\left\{ 0,1\right\} $
\end_inset

, our hypothesis should satisfy 
\begin_inset Formula $0\leq h_{x}\left(\boldsymbol{\theta}\right)\leq1$
\end_inset

.
 Using the 
\series bold
Sigmoid Function
\series default
 (or 
\series bold
Logistic Function
\series default
) on our linear regression hypothesis:
\begin_inset Formula 
\begin{eqnarray*}
z & = & \boldsymbol{\theta}^{T}\mathbf{x}\\
\therefore h_{\theta}\left(x\right) & = & \frac{1}{1+e^{-z}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week3/Chapter4/sigmoid-function.png
	lyxscale 30
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The sigmoid function; this has aymptotes at 
\begin_inset Formula $+1$
\end_inset

 and 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Interpretation of the 
\begin_inset Formula $h_{\theta}\left(x\right)$
\end_inset


\end_layout

\begin_layout Standard
We interpret 
\begin_inset Formula $h_{\theta}\left(x\right)$
\end_inset

 as 
\series bold
\emph on

\begin_inset Quotes eld
\end_inset

The estimated probabilty that 
\begin_inset Formula $y=1$
\end_inset

 on input 
\begin_inset Formula $x$
\end_inset


\begin_inset Quotes erd
\end_inset


\series default
\emph default
 (e.g.
 
\begin_inset Formula $h_{\theta}\left(x\right)=0.7\%$
\end_inset

 indicates we can tell the patient they have a 
\begin_inset Formula $70\,\%$
\end_inset

 chance of tumour being malignant); that is 
\begin_inset Formula $h_{\theta}\left(x\right)=\mathbb{P}\left(y=1;\mathbf{x},\boldsymbol{\theta}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $y\in\left\{ 0,1\right\} $
\end_inset

, we deduce that 
\begin_inset Formula $\mathbb{P}\left(y=0;\mathbf{x},\boldsymbol{\theta}\right)+\mathbb{P}\left(y=1;\mathbf{x},\boldsymbol{\theta}\right)=1$
\end_inset

 and so 
\series bold
\emph on

\begin_inset Quotes eld
\end_inset

The estimated probabilty that 
\begin_inset Formula $y=0$
\end_inset

 on input 
\begin_inset Formula $x$
\end_inset


\begin_inset Quotes erd
\end_inset


\series default
\emph default
 is 
\begin_inset Formula $1-h_{\theta}\left(x\right)$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Decision Boundary
\end_layout

\begin_layout Standard
In order to get our discrete 
\begin_inset Formula $0$
\end_inset

 or 
\begin_inset Formula $1$
\end_inset

 classification, we can translate the output of the hypothesis function
 via:
\begin_inset Formula 
\begin{eqnarray*}
h_{\theta}\left(x\right)\geq0.5\iff z\geq0 & \mapsto & y=1\\
h_{\theta}\left(x\right)<0.5\iff z<0 & \mapsto & y=0
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Definition
Th
\series bold
e decision boundary 
\series default
is the line that separates the area where 
\begin_inset Formula $y=0$
\end_inset

 and where 
\begin_inset Formula $y=1$
\end_inset

, that is created by our hypothesis function.
\end_layout

\begin_layout Remark*
The input to the sigmoid function 
\begin_inset Formula $z=\boldsymbol{\theta}^{T}\mathbf{x}$
\end_inset

 need not be linear, and could be a function that describes any shape that
 fits our data.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week3/Chapter4/decision-boundary.png
	lyxscale 50
	scale 50

\end_inset


\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week3/Chapter4/decision-boundary-circle.png
	lyxscale 50
	scale 50

\end_inset


\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week3/Chapter4/decision-boundary-random_shape.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Example decision boundaries: on the left, we use the usual (linear) 
\begin_inset Formula $h_{θ}\left(x\right)=g\left(θ_{0}+θ_{1}x_{1}+θ_{2}x_{2}\right)$
\end_inset

; in the middle, we could have e.g.
 
\begin_inset Formula $h_{θ}\left(x\right)=g\left(θ_{0}+θ_{1}x_{1}+θ_{3}x_{1}^{2}+θ_{4}x_{2}^{2}\right)$
\end_inset

 so that 
\begin_inset Formula $\boldsymbol{\theta}^{T}=\left[-1,0,0,1,1\right]$
\end_inset

 and 
\begin_inset Formula $y=1$
\end_inset

 if 
\begin_inset Formula $-1+x_{1}^{2}+x_{2}^{2}\geq0\Rightarrow x_{1}^{2}+x_{2}^{2}\geq1$
\end_inset

; so the right, we see that more complex decision boundaries can be found
 using higher order polynomial terms.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Binary Classification Problem: Evaluation (Cost function)
\end_layout

\begin_layout Standard
We cannot use the same cost function that we use for linear regression because
 the logistic function will cause the output to be wavy, causing 
\emph on
many local optima
\emph default
 (i.e.
 it will not be a convex function).
 Thus the cost function for learning the parameters 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J\left(\boldsymbol{\theta}\right)=\frac{1}{m}\sum_{i=1}^{m}\mbox{Cost}\left(h_{\theta}\left(x\right),y\right)\mbox{ where }\mbox{Cost}\left(h_{\theta}\left(x\right),y\right)=\begin{cases}
-\log\left(h_{\theta}\left(x\right)\right) & \mbox{ if }y=1\\
-\log\left(1-h_{\theta}\left(x\right)\right) & \mbox{ if }y=0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Note*
The further away our hypothesis, 
\begin_inset Formula $h_{\theta}\left(x\right)$
\end_inset

, is away from 
\begin_inset Formula $y$
\end_inset

, the larger the Cost function; conversly, when 
\begin_inset Formula $h_{\theta}\left(x\right)=y$
\end_inset

 our Cost function is 
\begin_inset Formula $0$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week3/Chapter4/cost-y=1.png
	lyxscale 40
	scale 40

\end_inset


\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week3/Chapter4/cost-y=0.png
	lyxscale 40
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The Cost function behaves in a similar manner to the one for linear regression,
 and ensures 
\begin_inset Formula $J\left(θ\right)$
\end_inset

 is convex for logistic regression.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Remark*
This cost function can be derived from statistics using the principle of
 maximum likelihood estimation.
\end_layout

\begin_layout Subsection
Binary Classification Problem: Optimisation
\end_layout

\begin_layout Subsubsection
Simplified cost function and gradient descent
\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $y\in\left\{ 0,1\right\} $
\end_inset

, we can write the Cost function as 
\begin_inset Formula $\mbox{Cost}\left(h_{\theta}\left(x\right),y\right)=-y\log\left(h_{\theta}\left(x\right)\right)-\left(1-y\right)\log\left(1-h_{\theta}\left(x\right)\right)$
\end_inset

; thus
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
J\left(\boldsymbol{\theta}\right) & = & \frac{1}{m}\sum_{i=1}^{m}\left[-y\log\left(h_{\theta}\left(x\right)\right)-\left(1-y\right)\log\left(1-h_{\theta}\left(x\right)\right)\right]\\
 & = & -\frac{1}{m}\left[y\log g\left(X\boldsymbol{\theta}^{T}\right)+\left(1-y\right)\log\left(1-g\left(XθT\right)\right)\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsubsection
Minimising the cost function for logistic regression
\end_layout

\begin_layout Standard
We can use the same algorithm for gradient descent as for 
\begin_inset CommandInset href
LatexCommand href
name "multivariate linear regression"
target "http://andyandy1992.github.io/MyMOOCs/courses/Coursera/Machine_Learning/Week2/3.Multivariate_linear_regression.pdf"

\end_inset

; the equation is the same but our definition for the hypothesis has changed.
\end_layout

\begin_layout Standard
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Algorithm

\series bold
Gradient Descent 
\end_layout

\begin_layout Algorithm
repeat until convergence {
\end_layout

\begin_layout Algorithm

\emph on
\begin_inset Formula $\boldsymbol{\theta}:=\boldsymbol{\theta}-\alpha\nabla J\left(\boldsymbol{\theta}\right)$
\end_inset


\end_layout

\begin_layout Algorithm

\emph on
}
\end_layout

\begin_layout Algorithm

\shape up
\emph on
for 
\begin_inset Formula $\alpha$
\end_inset

 some (positive) 
\series bold
learning
\series default
 
\series bold
rate
\series default
 (aka.
 step size) and some initial guess for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
\end_layout

\begin_layout Remark*
\begin_inset Formula $\nabla J\left(\boldsymbol{\theta}\right)=\left[\frac{\partial J\left(\boldsymbol{\theta}\right)}{\partial\theta_{0}},\frac{\partial J\left(\boldsymbol{\theta}\right)}{\partial\theta_{1}},...,\frac{\partial J\left(\boldsymbol{\theta}\right)}{\partial\theta_{n}}\right]^{T}$
\end_inset

 where 
\begin_inset Formula $\frac{\partial J\left(\boldsymbol{\theta}\right)}{\partial\theta_{j}}=\frac{1}{m}\sum_{i=1}^{m}x_{j}^{\left(i\right)}\left(h_{\boldsymbol{\theta}}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)$
\end_inset

.
 Equally 
\begin_inset Formula $\frac{\partial J\left(\boldsymbol{\theta}\right)}{\partial\theta_{j}}=\frac{1}{m}\mathbf{x}_{j}^{T}\left(g\left(X\boldsymbol{\theta}\right)-\mathbf{y}\right)$
\end_inset

, so 
\begin_inset Formula $\nabla J\left(\boldsymbol{\theta}\right)=\frac{1}{m}X^{T}\left(g\left(X\boldsymbol{\theta}\right)-\mathbf{y}\right)$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{\theta}:=\boldsymbol{\theta}-\frac{\alpha}{m}X^{T}\left(g\left(X\boldsymbol{\theta}\right)-\mathbf{y}\right)$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Remark*
Similarly, as before:
\end_layout

\begin_deeper
\begin_layout Itemize
We can monitor this gradient descent to check it's working correctly.
\end_layout

\begin_layout Itemize
Vectorized implementation is possible (and as before, more efficient).
\end_layout

\begin_layout Itemize
Feature scaling for gradient descent for logistic regression also applies.
\end_layout

\end_deeper
\begin_layout Subsubsection
Advanced optimisation
\end_layout

\begin_layout Standard
Previously we looked at gradient descent for minimizing the cost function.
 Here look at advanced concepts for minimizing the cost function for logistic
 regression.
\end_layout

\begin_layout Standard
Recall the aim of gradient descent is to minimise some cost function 
\begin_inset Formula $J\left(\boldsymbol{\theta}\right)$
\end_inset

.
 Thus we need to write code which can take 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

 as input and compute the following:
\end_layout

\begin_layout Itemize
\begin_inset Formula $J\left(\boldsymbol{\theta}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Partial derivatives of 
\begin_inset Formula $J\left(\boldsymbol{\theta}\right)$
\end_inset

 w.r.t.
 
\begin_inset Formula $j$
\end_inset

 (for 
\begin_inset Formula $j=0,...,n$
\end_inset

).
\end_layout

\begin_layout Standard

\series bold
Conjugate gradient
\series default
, 
\series bold
BFGS
\series default
 (Broyden-Fletcher-Goldfarb-Shanno or 
\begin_inset Quotes eld
\end_inset

Big Friendly Giants Steps
\begin_inset Quotes erd
\end_inset

) and 
\series bold
L-BFGS
\series default
 (Limited memory - BFGS) are more sophisticated, faster ways to optimize
 theta instead of using gradient descent.
\end_layout

\begin_layout Standard
Advantages:
\end_layout

\begin_layout Itemize
No need to manually pick alpha (learning rate).
\end_layout

\begin_deeper
\begin_layout Itemize
Have a clever inner loop (line search algorithm) which tries a bunch of
 alpha values and picks a good one.
\end_layout

\end_deeper
\begin_layout Itemize
Often faster than gradient descent.
\end_layout

\begin_deeper
\begin_layout Itemize
Do more than just pick a good learning rate.
\end_layout

\end_deeper
\begin_layout Itemize
Can be used successfully without understanding their complexity
\end_layout

\begin_layout Standard
Disadvantages:
\end_layout

\begin_layout Itemize
Could make debugging more difficult.
\end_layout

\begin_layout Itemize
Different libraries may use different implementations - may hit performance.
\end_layout

\begin_layout Subsection
Multiclass classification problems
\end_layout

\begin_layout Standard
We now look at getting logistic regression for multiclass classification
 (where 
\begin_inset Formula $y\in\left\{ 0,1,...,n\right\} $
\end_inset

) using 
\series bold
one vs.
 all
\series default
 (or 
\series bold
one vs.
 rest
\series default
).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week3/Chapter4/binary_vs_mulitclass.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Binary classification vs Multi-class classification.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
One vs.
 rest classification make binary classification work for multiclass classificati
on:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week3/Chapter4/one_vs_all.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
One vs.
 all algorithm (3-class problem):
\begin_inset Newline newline
\end_inset

1.
 Split the training set into 3 separate binary classification problems (i.e.
 create a new fake training set).
\begin_inset Newline newline
\end_inset

2.
 Train logistic regression classifier on each sub problem:
\begin_inset Newline newline
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

a.
 
\emph on
Triangle vs.
 Rest:
\emph default
 Train logistic regression classifier, 
\begin_inset Formula $h_{\theta}^{\left(1\right)}\left(x\right)=\mathbb{P}\left(y=1\mid\mathbf{x}^{\left(1\right)};\theta\right)$
\end_inset

.
\begin_inset Newline newline
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

b.
 
\emph on
Square vs.
 Rest:
\emph default
 Train logistic regression classifier 
\begin_inset Formula $h_{\theta}^{\left(2\right)}\left(x\right)=\mathbb{P}\left(y=1\mid\mathbf{x}^{\left(2\right)};\theta\right)$
\end_inset

.
\begin_inset Newline newline
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

c.
 
\emph on
Crosses vs.
 Rest:
\emph default
 Train logistic regression classifier 
\begin_inset Formula $h_{\theta}^{\left(3\right)}\left(x\right)=\mathbb{P}\left(y=1\mid\mathbf{x}^{\left(3\right)};\theta\right)$
\end_inset

.
\begin_inset Newline newline
\end_inset

3.
 The prediction is then 
\begin_inset Formula $\max_{i}\left(h_{\theta}^{\left(i\right)}\left(x\right)\right)$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
