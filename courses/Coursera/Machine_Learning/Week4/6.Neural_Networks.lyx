#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams-bytype
theorems-ams-extended-bytype
theorems-sec-bytype
eqs-within-sections
figs-within-sections
tabs-within-sections
\end_modules
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format pdf2
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered true
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "urlcolor=blue"
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 2
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\boxbgcolor #ffff7f
\branch How many Nobel Prizes?
\selected 1
\filename_suffix 0
\color #faf0e6
\end_branch
\branch Guess is 150
\selected 0
\filename_suffix 0
\color #faf0e6
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 1cm
\headsep 1cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 3
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{section}{4}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Regularisation
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Abstract

\emph on
Regularisation is designed to address the problem of overfitting
\emph default
.
 
\series bold
High bias 
\series default
or 
\series bold
underfitting 
\series default
is when the form of our hypothesis maps poorly to the trend of the data.
 It is usually caused by a function that is too simple or uses too few features
 (e.g.
 using a linear regression hypothesis when clearly the data appear quadratic).
\end_layout

\begin_layout Abstract
At the other extreme, 
\series bold
overfitting 
\series default
or 
\series bold
high variance 
\series default
is caused by a hypothesis function that fits the available data but does
 not generalize well to predict new data.
 It is usually caused by a complicated function that creates a lot of unnecessar
y curves and angles unrelated to the data.
\end_layout

\begin_layout Abstract
This terminology is applied to both linear and logistic regression.
\end_layout

\begin_layout Abstract
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\paragraph_spacing onehalf
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week3/Chapter5/underfiting-good_fit-overfitting.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Underfitting vs.
 
\begin_inset Quotes eld
\end_inset

a good fit
\begin_inset Quotes erd
\end_inset

 vs.
 Overfitting
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Abstract
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week3/Chapter5/UFvsGFvsOF-logistic_regression.png
	lyxscale 40
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Logistic regression: Underfitting vs.
 
\begin_inset Quotes eld
\end_inset

a good fit
\begin_inset Quotes erd
\end_inset

 vs.
 Overfitting
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Addressing the issue of overfitting
\end_layout

\begin_layout Standard
There are 2 main options for addressing the issue of overfitting:
\end_layout

\begin_layout Enumerate

\series bold
\emph on
Reduce the number of features.

\emph default
 
\series default
Thi
\emph on
s 
\emph default
can be achieved by:
\end_layout

\begin_deeper
\begin_layout Enumerate
M
\emph on
anually 
\emph default
selecting which features to keep.
\end_layout

\begin_layout Enumerate
Using a model selection algorithm (studied later in the course).
\end_layout

\begin_layout Standard
Of course, reducing the number of features ultimately results in losing
 some information (we ideally select those features which minimize data
 loss).
\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
\emph on
Regularization
\end_layout

\begin_deeper
\begin_layout Enumerate
Keep all the features, but reduce the parameters 
\begin_inset Formula $\theta_{j}$
\end_inset

.
\end_layout

\begin_layout Enumerate
This works well when we have a lot of features, each of which contributes
 a bit to predicting 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
Regularisation works well when we have a lot of slightly useful features.
\end_layout

\begin_layout Subsection
Cost function modification for regularisation
\end_layout

\begin_layout Standard
To reduce the weights (or parameters) so that the following hypothesis function
 is more quadratic:
\begin_inset Formula 
\[
\theta_{0}+\theta_{1}x+\theta_{2}x^{2}+\theta_{3}x^{3}+\theta_{4}x^{4}
\]

\end_inset


\end_layout

\begin_layout Standard
we can eliminate the influence of 
\begin_inset Formula $\theta_{3}x^{3}$
\end_inset

 and 
\begin_inset Formula $\theta_{4}x^{4}$
\end_inset

, without actually getting rid of these features or changing the form of
 our hypothesis.
 We can instead modify our cost function:
\begin_inset Formula 
\[
\min_{\theta}\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}+1000\theta_{3}^{2}+1000\theta_{4}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week3/Chapter5/effect_of_modified_cost.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
This modification of our cost function help to penalise 
\begin_inset Formula $\theta_{3}x^{3}$
\end_inset

 and 
\begin_inset Formula $\theta_{4}x^{4}$
\end_inset

, so here we end up with 
\begin_inset Formula $\theta_{3}$
\end_inset

 and 
\begin_inset Formula $\theta_{4}$
\end_inset

 being close to zero (because the constants are massive) - so we're basically
 left with a quadratic function.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
More generally, for regularisation we can modify the cost function:
\end_layout

\begin_layout Definition
The regularised objective function is defined as
\begin_inset Formula 
\[
\min_{\theta}\frac{1}{2m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}+\lambda\sum_{j=1}^{n}\theta_{j}^{2}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
By convention we don't penalize 
\begin_inset Formula $\theta_{0}$
\end_inset

 - minimisation is from 
\begin_inset Formula $\theta_{1}$
\end_inset

 (but in practice including 
\begin_inset Formula $\theta_{0}$
\end_inset

 has negligible effect).
\end_layout

\begin_layout Definition
\begin_inset Formula $\lambda$
\end_inset

 is the 
\series bold
regularisation parameter
\series default
, which determines how much the costs of our theta parameters are inflated.
 
\end_layout

\begin_layout Remark*
\begin_inset Formula $\lambda$
\end_inset

 controls a trade off between our two goals:
\end_layout

\begin_deeper
\begin_layout Enumerate
Fitting the training set well; we can smooth/make less curvy the output
 of our hypothesis function to reduce overfitting.
\end_layout

\begin_layout Enumerate
Keeping parameters small; WARNING: if 
\begin_inset Formula $\lambda\gg1$
\end_inset

 it may smooth out the function too much (i.e.
 effectively removing all terms) and cause underfitting.
\end_layout

\end_deeper
\begin_layout Standard
We look at some automatic ways to select 
\begin_inset Formula $\lambda$
\end_inset

 later in the course.
\end_layout

\begin_layout Subsection
Optimisation: Gradient descent for regularisation
\end_layout

\begin_layout Standard
Because the 
\begin_inset Formula $\theta_{0}$
\end_inset

 term is excluded from regularisation, the algorithm is modified in the
 following (again using calculus):
\end_layout

\begin_layout Standard
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Algorithm

\series bold
Gradient Descent for 
\series default
regularisation
\end_layout

\begin_layout Algorithm
repeat until convergence {
\end_layout

\begin_layout Algorithm
\begin_inset Formula $\theta_{0}:=\theta_{0}-\frac{\alpha}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{0}^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Algorithm

\emph on
\begin_inset Formula $\theta_{j}:=\theta_{j}-\frac{\alpha}{m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}+\lambda\theta_{j}\right],\ \mbox{for }j=1,2,...,n$
\end_inset


\end_layout

\begin_layout Algorithm

\emph on
}
\end_layout

\begin_layout Algorithm

\shape up
\emph on
for 
\begin_inset Formula $\alpha$
\end_inset

 some (positive) 
\series bold
learning
\series default
 
\series bold
rate
\series default
 (aka.
 step size) and some initial guess for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Remark*
Grouping the 
\begin_inset Formula $\theta_{j}$
\end_inset

 terms, we see that 
\begin_inset Formula 
\[
\theta_{j}=\theta_{j}\left(1-\alpha\frac{\lambda}{m}\right)-\frac{\alpha}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}
\]

\end_inset


\begin_inset Formula $\frac{\alpha}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset

 is exactly the same as the original gradient descent; the term 
\begin_inset Formula $\frac{\lambda\theta_{j}}{m}$
\end_inset

 performs our regularisation.
 Noting that 
\begin_inset Formula $\alpha$
\end_inset

 is small and 
\begin_inset Formula $m$
\end_inset

 is large, we see that typically 
\begin_inset Formula $\left(1-\alpha\frac{\lambda}{m}\right)<1$
\end_inset

, in particular 
\begin_inset Formula $0.95\leq\left(1-\alpha\frac{\lambda}{m}\right)\leq0.99$
\end_inset

.
 Thus, this in effect means 
\begin_inset Formula $\theta_{j}$
\end_inset

 gets multiplied by 
\begin_inset Formula $0.99$
\end_inset

 (reducing the value of 
\begin_inset Formula $\theta_{j}$
\end_inset

 by some amount on every update).
\end_layout

\begin_layout Subsection
Optimisation: Normal equations for regularisation
\end_layout

\begin_layout Standard
To add in regularisation to our normal equations, we use:
\begin_inset Formula 
\[
\boldsymbol{\theta}=\left(X^{T}X+{\color{red}\lambda L}\right)^{-1}X^{T}\mathbf{y}\mbox{, where }L=\left[\begin{array}{cccc}
0\\
 & 1\\
 &  & \ddots\\
 &  &  & 1
\end{array}\right]\in\mathbb{R}^{\left(n+1\right)\times\left(n+1\right)}
\]

\end_inset


\end_layout

\begin_layout Remark*
Intuitively, 
\begin_inset Formula $L$
\end_inset

 is essentially the identity matrix multiplied by a single real number 
\begin_inset Formula $\lambda$
\end_inset

 (not including 
\begin_inset Formula $x_{0}$
\end_inset

).
\end_layout

\begin_layout Subsection
R Example
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week3/Chapter5/prestige-preparation.png

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week3/Chapter5/regularisation_R.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Regularisation in 
\series bold
R
\series default
.
 Note the second line reads 
\family typewriter
> cv.fit <- cv.glmnet(as.matrix(prestige_train[,c(-4, -6)]), as.vector(prestige_trai
n[,4]), nlambda=100, alpha=0.7, family="gaussian")
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The 
\family typewriter
glmnet
\family default
 library provides a cross-validation test to automatically choose the better
 
\begin_inset Formula $\lambda$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week3/Chapter5/cv-lamba.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cross-validation plot for 
\begin_inset Formula $\lambda$
\end_inset

.
 It shows the best 
\begin_inset Formula $\lambda$
\end_inset

 with minimal-root, mean-square error.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Neural Networks.
 
\end_layout

\begin_layout Section
Practical advice for applying learning algorithms: How to develop, debugging,
 feature/model design, setting up experiment structure.
 
\end_layout

\begin_layout Section
Support Vector Machines (SVMs) and the intuition behind them.
 
\end_layout

\begin_layout Section
Unsupervised learning: clustering and dimensionality reduction.
\end_layout

\begin_layout Section
Anomaly detection.
 
\end_layout

\begin_layout Section
Recommender systems.
 
\end_layout

\begin_layout Section
Large-scale machine learning.
\end_layout

\begin_layout Section
An example of an application of machine learning.
 
\end_layout

\end_body
\end_document
