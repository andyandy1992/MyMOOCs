#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams-bytype
theorems-ams-extended-bytype
theorems-sec-bytype
eqs-within-sections
figs-within-sections
tabs-within-sections
\end_modules
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format pdf2
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered true
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "urlcolor=blue"
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 2
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\boxbgcolor #ffff7f
\branch How many Nobel Prizes?
\selected 1
\filename_suffix 0
\color #faf0e6
\end_branch
\branch Guess is 150
\selected 0
\filename_suffix 0
\color #faf0e6
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 1cm
\headsep 1cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 3
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{section}{5}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Neural Networks
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Abstract
This chapter introduces how we can dealing with non-linear hypotheses with
 neural network.
 Although because I have completed the dedicated course 
\begin_inset Quotes eld
\end_inset

Neural networks for machine learning
\begin_inset Quotes erd
\end_inset

 on Coursera, please see this course for more explicit details (to avoid
 duplication).
 This chapter predominately discusses neural networks for classification.
\end_layout

\begin_layout Subsection
Representation
\end_layout

\begin_layout Notation

\series bold
Activation units and vector representation of neural networks
\end_layout

\begin_layout Notation
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week4/Chapter6/nn-notation.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A network with 
\begin_inset Formula $s_{j}$
\end_inset

 units in layer 
\begin_inset Formula $j$
\end_inset

, 
\begin_inset Formula $s_{j+1}$
\end_inset

 units in layer 
\begin_inset Formula $j+1$
\end_inset

, then 
\begin_inset Formula $\Theta^{\left(j\right)}$
\end_inset

 will be of dimension 
\begin_inset Formula $s_{j+1}\times\left(s_{j}+1\right)$
\end_inset

.
 The process of 
\series bold
\emph on
computing
\emph default
 
\begin_inset Formula $h_{\Theta}\left(x\right)$
\end_inset

 
\series default
is called
\series bold
 forward propagation
\series default
 (since we must start with the activations of the input units, and then
 we forward propagate these to compute the activations in the hidden layer
 and so on).
 
\emph on
(c.f back propagation, where we are given 
\begin_inset Formula $h_{\Theta}\left(x\right)$
\end_inset

 and want to learn the weights/parameters.)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Remark*
Looking at how we compute each activation unit, 
\begin_inset Formula $a_{i}^{\left(j\right)}$
\end_inset

, we notice that what this neural network is just doing logistic regression
 (with 
\series bold

\begin_inset Quotes eld
\end_inset

learned
\begin_inset Quotes erd
\end_inset

 inputs
\series default
).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week4/Chapter6/nn-adding_complexity.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Neural network are used to learn more complex functions with each layer.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Multiple output units: One-vs-all
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week4/Chapter6/nn-one_vs_all.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
In the one vs.
 all (equal to 4 here) representation, instead of letting 
\begin_inset Formula $y\in\left\{ 0,1\right\} $
\end_inset

, we let 
\begin_inset Formula $y\in\left\{ \left[1,0,0,0\right]^{T},\left[0,1,0,0\right]^{T},\left[0,0,1,0\right]^{T},\left[0,0,0,1\right]^{T}\right\} $
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Evaluation: Cost Function
\end_layout

\begin_layout Notation
\begin_inset Formula $L:=$
\end_inset

total # layers in network.
 
\begin_inset Formula $s_{l}=$
\end_inset

# units (excluding bias unit) in layer 
\begin_inset Formula $l$
\end_inset

 (
\begin_inset Formula $s_{1}:=$
\end_inset

# units in input layer, 
\begin_inset Formula $s_{L}:=$
\end_inset

# units in output layer).
\end_layout

\begin_layout Standard
The cost function for neural networks is a generalisation of the cost function
 used from logistic regression (with regularisation); so 
\begin_inset Formula $h_{\Theta}\left(x\right)\in\mathbb{R}^{K}$
\end_inset

 (i.e.
 
\begin_inset Formula $\left(h_{\Theta}\left(x\right)\right)_{i}=$
\end_inset

probability that the 
\begin_inset Formula $i^{th}$
\end_inset

 output is 
\begin_inset Formula $1$
\end_inset

), and we take the sum over these 
\begin_inset Formula $K$
\end_inset

 output units in the cost function:
\begin_inset Formula 
\[
J\left(\Theta\right)=-\frac{1}{m}\left[\sum_{i=1}^{m}\sum_{k=1}^{K}y_{k}^{\left(i\right)}\log\left(h_{\Theta}\left(x^{\left(i\right)}\right)\right)_{k}+\left(1-y_{k}^{\left(i\right)}\right)\log\left(1-\left(h_{\Theta}\left(x^{\left(i\right)}\right)\right)_{k}\right)\right]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^{s_{l+1}}\left(\Theta_{ji}^{\left(l\right)}\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Subsection
Optimisation: Backpropagation
\end_layout

\begin_layout Standard
To find 
\begin_inset Formula $\min_{\Theta}J\left(\Theta\right)$
\end_inset

 using gradient descent or one of the advanced optimisation algorithms,
 we need to compute 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

 and 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{ij}^{\left(l\right)}}J\left(\Theta\right)$
\end_inset

 (recall 
\begin_inset Formula $\Theta_{ij}^{\left(l\right)}\in\mathbb{R}$
\end_inset

).
\end_layout

\begin_layout Subsubsection
Gradient Computation for the case of one training example
\end_layout

\begin_layout Standard
Computing the gradient in the case of one training example 
\begin_inset Formula $\left(\mathbf{x},\mathbf{y}\right)$
\end_inset

, we see that:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week4/Chapter6/gradient-computation-1-training-example.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
Step 1 - Forward Propagation:
\series default
 Calculating the activations 
\begin_inset Formula $\mathbf{a}^{\left(i\right)}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week4/Chapter6/backpropagation-1-training-example.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
Step 2 - Backpropagation:
\series default
 Calculating the gradients 
\begin_inset Formula $\boldsymbol{\mathbf{\delta}}^{\left(i\right)}$
\end_inset

.
 Note the 
\begin_inset Formula $.*$
\end_inset

 denotes element-wise multiplication and we do not calculate 
\begin_inset Formula $\boldsymbol{\delta}^{\left(1\right)}$
\end_inset

 as the first layer does not have any error associated with it (it's just
 the features we observed in our training set).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Remark*
When the cost function is 
\series bold
not 
\series default
regularised (i.e.
 
\begin_inset Formula $\lambda=0$
\end_inset

), it can be shown that 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{ij}^{\left(l\right)}}J\left(\Theta\right)=a_{j}^{\left(l\right)}\delta_{i}^{\left(l+1\right)}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Gradient Computation for 
\begin_inset Formula $m$
\end_inset

 training examples
\end_layout

\begin_layout Standard
For a training set 
\begin_inset Formula $\left\{ \left(x^{\left(1\right)},y^{\left(1\right)}\right),...,\left(x^{\left(m\right)},y^{\left(m\right)}\right)\right\} $
\end_inset

:
\end_layout

\begin_layout Standard

\series bold
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Algorithm

\series bold
Backpropagation Algorithm
\end_layout

\begin_layout Algorithm
Set 
\begin_inset Formula $\Delta_{ij}^{\left(l\right)}=0$
\end_inset

, 
\begin_inset Formula $\forall l,i,j$
\end_inset

.
\end_layout

\begin_layout Algorithm
For 
\begin_inset Formula $i=1,...,m$
\end_inset

:
\end_layout

\begin_layout Algorithm
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

Set 
\begin_inset Formula $a^{\left(1\right)}=x^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Algorithm
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

Perform forward propagation to computer 
\begin_inset Formula $a^{\left(l\right)}$
\end_inset

 for 
\begin_inset Formula $l=2,3,...,L$
\end_inset


\end_layout

\begin_layout Algorithm
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

Using 
\begin_inset Formula $y^{\left(i\right)}$
\end_inset

, compute 
\begin_inset Formula $\delta^{\left(L\right)}=a^{\left(L\right)}-y^{\left(i\right)}$
\end_inset


\end_layout

\begin_layout Algorithm
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

Compute 
\begin_inset Formula $\delta^{\left(L-1\right)}$
\end_inset

,
\begin_inset Formula $\delta^{\left(L-2\right)}$
\end_inset

,...,
\begin_inset Formula $\delta^{\left(2\right)}$
\end_inset


\end_layout

\begin_layout Algorithm
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Formula $\Delta_{ij}^{\left(l\right)}:=\Delta_{ij}^{\left(l\right)}+a_{j}^{\left(l\right)}\delta_{i}^{\left(l+1\right)}$
\end_inset

 (or vectorised form 
\begin_inset Formula $\boldsymbol{\Delta}^{\left(l\right)}:=\boldsymbol{\Delta}^{\left(l\right)}+\delta^{\left(l+1\right)}\left(a^{\left(l\right)}\right)^{T}$
\end_inset

)
\end_layout

\begin_layout Algorithm
\begin_inset Formula $D_{ij}^{\left(l\right)}:=\begin{cases}
\frac{1}{m}\Delta_{ij}^{\left(l\right)}+\lambda\Theta_{ij}^{\left(l\right)} & \mbox{ if }j\neq0\\
\frac{1}{m}\Delta_{ij}^{\left(l\right)} & \mbox{ if }j=0
\end{cases}$
\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Remark*
For quite a complicate proof, it can be shown that the partial derivatives
 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{ij}^{\left(l\right)}}J\left(\Theta\right)=D_{ij}^{\left(l\right)}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Practical advice for applying learning algorithms: How to develop, debugging,
 feature/model design, setting up experiment structure.
 
\end_layout

\begin_layout Section
Support Vector Machines (SVMs) and the intuition behind them.
 
\end_layout

\begin_layout Section
Unsupervised learning: clustering and dimensionality reduction.
\end_layout

\begin_layout Section
Anomaly detection.
 
\end_layout

\begin_layout Section
Recommender systems.
 
\end_layout

\begin_layout Section
Large-scale machine learning.
\end_layout

\begin_layout Section
An example of an application of machine learning.
 
\end_layout

\end_body
\end_document
