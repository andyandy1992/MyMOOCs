#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams-bytype
theorems-sec-bytype
theorems-ams-extended-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format pdf2
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 1cm
\headsep 1cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 3
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard

\lang british
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{section}{1}
\end_layout

\end_inset


\end_layout

\begin_layout Section

\lang british
Univariate linear regression
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Subsection

\lang british
Representation: Single Features
\end_layout

\begin_layout Notation*
For 
\lang british
univariate linear regression, we have
\end_layout

\begin_layout Notation*

\lang british
\begin_inset Formula $m:=$
\end_inset

# training examples, 
\begin_inset Formula $x:=$
\end_inset

input variable (or features), 
\begin_inset Formula $y:=$
\end_inset

output (or target) variable
\end_layout

\begin_layout Notation*

\lang british
\begin_inset Formula $\left(x,y\right):=$
\end_inset

one training example
\end_layout

\begin_layout Notation*

\lang british
\begin_inset Formula $\left(x^{\left(i\right)},y^{\left(i\right)}\right):=i^{th}$
\end_inset

 training example
\end_layout

\begin_layout Standard

\lang british
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\lang british
\begin_inset Graphics
	filename ../Images/Week1/Chapter2/Model representation (univariate linear regression).png
	lyxscale 60
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout

\lang british
\begin_inset Caption Standard

\begin_layout Plain Layout

\emph on
\lang british
Representation
\emph default
:
\emph on
 
\emph default
In univariate linear regression, our 
\series bold
hypothesis function
\series default
 has the general form 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x$
\end_inset

 .
 We determine the values for the parameters 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

 by running our 
\series bold
training set 
\series default
on a 
\series bold
learning algorithm
\series default
.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection

\lang british
Evaluation: Cost Function for univariate linear regression
\end_layout

\begin_layout Standard

\series bold
\lang british
Idea: 
\series default
We use a cost function for learning the parameters:
\series bold
 
\series default
Choose 
\begin_inset Formula $\theta_{0},\theta_{1}$
\end_inset

 so that 
\begin_inset Formula $h_{\theta}\left(x\right)$
\end_inset

 is close to 
\begin_inset Formula $y$
\end_inset

 for our training set.
 That is, minimise:
\begin_inset Formula 
\[
\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}\iff\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Note*

\lang british
\begin_inset Formula $\frac{1}{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset

 is equal to the means of the squares of the residual, 
\begin_inset Formula $\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)$
\end_inset

.
\end_layout

\begin_layout Remark*

\lang british
The mean is halved as a convenience of the gradient descent computation
 (as the derivative term of the square function will cancel out with the
 
\begin_inset Formula $\frac{1}{2}$
\end_inset

; note also that dividing by a constant 
\begin_inset Formula $\frac{1}{2m}$
\end_inset

 is a linear operator, and so it does not effect which parameters 
\begin_inset Quotes eld
\end_inset

score
\begin_inset Quotes erd
\end_inset

 best).
\end_layout

\begin_layout Definition

\lang british
The 
\series bold
cost 
\series default
(
\series bold
squared error
\series default
)
\series bold
 function for univariate linear regression
\series default
 of a training set is 
\begin_inset Formula 
\[
J\left(\theta_{0},\theta_{1}\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Definition

\lang british
We seek 
\begin_inset Formula $\min_{h_{\theta}}J\left(\theta_{0},\theta_{1}\right)\iff\min_{\theta_{0},\theta_{1}}J\left(\theta_{0},\theta_{1}\right)$
\end_inset

.
\end_layout

\begin_layout Note*
The cost function uses 
\emph on
all
\emph default
 the data in the training set.
\end_layout

\begin_layout Subsubsection

\lang british
Cost Function Intuition
\end_layout

\begin_layout Example*

\series bold
\emph on
Case 
\series default
\emph default
\lang british

\begin_inset Formula $\theta_{0}=0$
\end_inset


\end_layout

\begin_layout Example*

\lang british
Consider a simplified version of the straight line model, with 
\begin_inset Formula $\theta_{0}=0$
\end_inset

:
\begin_inset Formula 
\[
h_{\theta}\left(x\right)=\theta_{1}x\mbox{ and }J\left(\theta_{0},\theta_{1}\right)=J\left(\theta_{1}\right)
\]

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\lang british
\begin_inset Graphics
	filename ../Images/Week1/Chapter2/Cost Function Intuition.png
	lyxscale 60
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout

\lang british
\begin_inset Caption Standard

\begin_layout Plain Layout

\lang british
We get the following lines for:
\begin_inset Newline newline
\end_inset


\begin_inset Formula $\theta_{1}=1$
\end_inset

: 
\color cyan
Cyan
\color inherit
 line
\begin_inset Newline newline
\end_inset


\begin_inset Formula $\theta_{1}=0.5$
\end_inset

: 
\color magenta
Magenta
\color inherit
 line
\begin_inset Newline newline
\end_inset


\begin_inset Formula $\theta_{1}=0$
\end_inset

: 
\color blue
Blue
\color inherit
 line
\begin_inset Newline newline
\end_inset

Observe 
\begin_inset Formula $\theta_{1}=1$
\end_inset

 minimises 
\begin_inset Formula $J\left(\theta_{1}\right)$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Example*

\series bold
\emph on
More general case
\end_layout

\begin_layout Example*

\lang british
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\lang british
\begin_inset Graphics
	filename ../Images/Week1/Chapter2/Bowl Plot.png
	lyxscale 60
	scale 50

\end_inset


\begin_inset Graphics
	filename ../Images/Week1/Chapter2/Contour Plot.png
	lyxscale 60
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout

\lang british
\begin_inset Caption Standard

\begin_layout Plain Layout

\lang british
3D representation and it's contour plot.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Example*

\lang british
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\lang british
\begin_inset Graphics
	filename ../Images/Week1/Chapter2/Cost Function Intuition (2 parameters) 1.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\lang british
\begin_inset Graphics
	filename ../Images/Week1/Chapter2/Cost Function Intuition (2 parameters) 2.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout

\lang british
\begin_inset Caption Standard

\begin_layout Plain Layout

\lang british
The closer the values of 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $\theta_{1}$
\end_inset

 to the minimum (in the contour plot), the better the line of best fit is.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Remark*

\lang british
We seek an algorithm that efficiently minimises the cost function 
\begin_inset Formula $J$
\end_inset

.
\end_layout

\begin_layout Subsection

\lang british
Optimisation: Gradient Descent univariate linear regression
\end_layout

\begin_layout Standard

\series bold
\lang british
Idea: 
\series default
You're at the top of a hill and you seek a route to the bottom by 
\emph on
always choosing the
\emph default
 
\emph on
lowest
\emph default
 
\emph on
step
\emph default
.
\end_layout

\begin_layout Standard

\lang british
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Algorithm

\series bold
\lang british
Gradient Descent 
\end_layout

\begin_layout Algorithm

\lang british
repeat until convergence {
\end_layout

\begin_layout Algorithm

\emph on
\lang british
\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 
\shape up
(simultaneously update 
\begin_inset Formula $j=0$
\end_inset

 and 
\begin_inset Formula $j=1$
\end_inset

)
\end_layout

\begin_layout Algorithm

\emph on
\lang british
}
\end_layout

\begin_layout Algorithm

\shape up
\emph on
\lang british
for 
\begin_inset Formula $\alpha$
\end_inset

 some (positive) 
\series bold
learning
\series default
 
\series bold
rate
\series default
 (aka.
 step size) and some initial guess for 
\begin_inset Formula $\theta_{0},\theta_{1}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Note*

\series bold
\lang british
Simultaneously
\series default
 
\series bold
update
\series default
 means:
\end_layout

\begin_layout Note*

\family typewriter
\lang british
temp0 
\begin_inset Formula $:=\theta_{0}-\alpha\frac{\partial}{\partial\theta_{0}}J\left(\theta_{0},\theta_{1}\right)$
\end_inset

; temp1 
\begin_inset Formula $:=\theta_{1}-\alpha\frac{\partial}{\partial\theta_{1}}J\left(\theta_{0},\theta_{1}\right)$
\end_inset


\end_layout

\begin_layout Note*

\lang british
\begin_inset Formula $\theta_{0}:=$
\end_inset


\family typewriter
temp0
\family default
; 
\begin_inset Formula $\theta_{1}:=$
\end_inset


\family typewriter
temp1
\family default
.
\end_layout

\begin_layout Remark*
\begin_inset Formula $\frac{\partial}{\partial\theta_{j}}J\left(\theta_{0},\theta_{1}\right)$
\end_inset

 is equal to the slope of the tangent.
\end_layout

\begin_deeper
\begin_layout Remark*

\series bold
\emph on
\lang british
WARNING using gradient descent
\end_layout

\begin_layout Remark*

\lang british
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\lang british
\begin_inset Graphics
	filename ../Images/Week1/Chapter2/ProblemsWithGradientDecent.png
	lyxscale 60
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout

\lang british
\begin_inset Caption Standard

\begin_layout Plain Layout

\lang british
Where you start (your initial guess) can affect which (local) minimum you
 arrive at, and this may not necessarily be the global minimum.
 Observe if we are at a local minimum, one step of gradient descent does
 nothing.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Subsubsection
Intuition behind gradient descent
\end_layout

\begin_layout Standard

\lang british
Consider a 1D case: 
\begin_inset Formula $J\left(\theta_{1}\right)$
\end_inset

 for 
\begin_inset Formula $\theta_{1}\in\mathbb{R}$
\end_inset

, so 
\begin_inset Formula $\theta_{1}:=\theta_{1}-\alpha\frac{d}{d\theta_{1}}J\left(\theta_{1}\right)$
\end_inset

.
 Thus
\end_layout

\begin_layout Standard

\lang british
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\lang british
\begin_inset Graphics
	filename ../Images/Week1/Chapter2/Gradient descent - derivative term.png
	lyxscale 60
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout

\lang british
\begin_inset Caption Standard

\begin_layout Plain Layout

\lang british
Note, the derivative term always has the correct sign for sending us in
 the correct direction (towards the minimum).
 Also note, gradient descent automatically takes smaller steps with time
 (as we approach the local minimum, the derivative get smaller), so there
 is no need to decrease 
\begin_inset Formula $\alpha$
\end_inset

 over time.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Remark*

\series bold
\emph on
\lang british
Choosing a sensible learning rate,
\begin_inset Formula $\alpha$
\end_inset


\emph default
.
\end_layout

\begin_layout Remark*

\lang british
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\lang british
\begin_inset Graphics
	filename ../Images/Week1/Chapter2/Gradient descent - the learning rate.png
	lyxscale 65
	scale 65

\end_inset


\end_layout

\begin_layout Plain Layout

\lang british
\begin_inset Caption Standard

\begin_layout Plain Layout

\lang british
TOP: if 
\begin_inset Formula $\alpha$
\end_inset

 is too 
\emph on
small
\emph default
, gradient descent can be slow.
\begin_inset Newline newline
\end_inset

BOTTOM: if 
\begin_inset Formula $\alpha$
\end_inset

 is too 
\emph on
large
\emph default
, gradient descent can overshoot the minimum and can fail to converge (and
 can even diverge).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection

\lang british
Applying the cost function for gradient descent - a first learning algorithm
 for linear regression
\end_layout

\begin_layout Standard

\lang british
\begin_inset Wrap figure
lines 0
placement o
overhang 0col%
width "50col%"
status open

\begin_layout Plain Layout
\noindent
\align center

\lang british
\begin_inset Graphics
	filename ../Images/Week1/Chapter2/Cost Function for Gradient Descent.png
	lyxscale 60
	scale 55

\end_inset


\end_layout

\begin_layout Plain Layout

\lang british
\begin_inset Caption Standard

\begin_layout Plain Layout

\lang british
Recall that a problem with general gradient descent is that initial conditions
 can affect the (local) minimum you arrive at; however 
\emph on
\color green
the Cost Function for gradient descent is 
\emph default
always
\emph on
 a bowl shape
\emph default
\color inherit
 (a 
\series bold
convex function
\series default
), which has 
\emph on
\color green
one global minimum
\emph default
\color inherit
.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\lang british
Observe by linearity of the derivative,
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial\theta_{j}}J\left(\theta_{0},\theta_{1}\right) & = & \frac{\partial}{\partial\theta_{j}}\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}\\
 & = & \frac{1}{2m}\frac{\partial}{\partial\theta_{j}}\sum_{i=1}^{m}\left(\theta_{0}+\theta_{1}x^{\left(i\right)}-y^{\left(i\right)}\right)^{2}
\end{eqnarray*}

\end_inset

Also by the chain rule, we have 
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial}{\partial\theta_{0}}J\left(\theta_{0},\theta_{1}\right) & = & \frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)\\
\frac{\partial}{\partial\theta_{1}}J\left(\theta_{0},\theta_{1}\right) & = & \frac{1}{m}\sum_{i=1}^{m}x^{\left(i\right)}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard

\lang british
Hence
\end_layout

\begin_layout Standard

\lang british
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Algorithm

\series bold
\lang british
Batch Gradient Descent
\series default
 
\end_layout

\begin_layout Algorithm

\lang british
repeat until convergence {
\end_layout

\begin_layout Algorithm

\lang british
\begin_inset Formula $\theta_{0}:=\theta_{0}-\alpha\left[\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)\right]$
\end_inset


\end_layout

\begin_layout Algorithm

\lang british
\begin_inset Formula $\theta_{1}:=\theta_{1}-\alpha\left[\frac{1}{m}\sum_{i=1}^{m}x^{\left(i\right)}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)\right]$
\end_inset


\end_layout

\begin_layout Algorithm

\shape up
\emph on
\lang british
(simultaneously update 
\begin_inset Formula $j=0$
\end_inset

 and 
\begin_inset Formula $j=1$
\end_inset

)
\end_layout

\begin_layout Algorithm

\emph on
\lang british
}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
\align center

\lang british
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\lang british
\begin_inset Graphics
	filename ../Images/Week1/Chapter2/Mixing Gradient Descent with the Cost Function.png
	lyxscale 60
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout

\lang british
\begin_inset Caption Standard

\begin_layout Plain Layout

\lang british
This results in a 
\emph on
fast approach
\emph default
 to the 
\emph on
global
\emph default
 minimum.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Remark*
The 
\begin_inset Quotes eld
\end_inset

Batch
\begin_inset Quotes erd
\end_inset

 refers to the fact that over each step we look at all the training data
 (each step compute over 
\begin_inset Formula $m$
\end_inset

 training examples).
 Some non-batch versions exist, which look at small data subsets (to use
 when 
\begin_inset Formula $m$
\end_inset

 is too large).
\end_layout

\end_body
\end_document
