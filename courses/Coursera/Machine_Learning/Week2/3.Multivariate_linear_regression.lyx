#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams-bytype
theorems-ams-extended-bytype
theorems-sec-bytype
eqs-within-sections
figs-within-sections
tabs-within-sections
\end_modules
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format pdf2
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 1cm
\headsep 1cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 3
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{section}{2}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Multivariate linear regression
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Abstract

\lang english
We now describe 
\lang british
multivariate linear regression, a
\lang english
 variate of the previous learning algorithm where we have multiple features
 (or inputs).
 We also introduce 
\lang british
the matrix notation of the cost function, and outline how gradient descent
 in performed practice
\lang english
.
 
\lang british
Finally, we describe an alternative method for solving multivariate linear
 regression problems, when 
\begin_inset Formula $n$
\end_inset

, the number of training features, is sufficiently small.
\end_layout

\begin_layout Subsection
Representation: Multiple Features
\end_layout

\begin_layout Notation*
\begin_inset Formula $n:=$
\end_inset

number of features,
\end_layout

\begin_layout Notation*
\begin_inset Formula $\mathbf{x}^{\left(i\right)}:=x^{\left(i\right)}=$
\end_inset

input (feature) of 
\begin_inset Formula $i^{th}$
\end_inset

 training example,
\end_layout

\begin_layout Notation*
\begin_inset Formula $x_{j}^{\left(i\right)}:=$
\end_inset

value of feature 
\begin_inset Formula $j$
\end_inset

 in 
\begin_inset Formula $i^{th}$
\end_inset

 training example.
\end_layout

\begin_layout Remark*
The more information we have about a data set, the better the predictions
 will be.
\end_layout

\begin_layout Example
Suppose now we have more features of a house on which to estimate the price
 of:
\end_layout

\begin_layout Example
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../Images/Week2/Chapter3/Multiple features of a house.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:1"

\end_inset

Here 
\begin_inset Formula $n=4$
\end_inset

, 
\begin_inset Formula $x^{\left(2\right)}=\left[1416,3,2,40\right]^{T}$
\end_inset

 and so 
\begin_inset Formula $x_{2}^{\left(2\right)}=3$
\end_inset

, for instance.
 So the hypothesis function extends to 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}+\theta_{4}x_{4}$
\end_inset

 or 
\begin_inset Formula $\left[\theta_{0},\theta_{1},\theta_{2},\theta_{3},\theta_{4}\right]\left[x_{0},x_{1},x_{2},x_{3},x_{4}\right]^{T}$
\end_inset

 i.e.
 
\begin_inset Formula $x_{0}=1$
\end_inset

, the intercept.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Remark*
The general hypothesis function for 
\begin_inset Formula $n$
\end_inset

-features (excluding the additional 
\begin_inset Formula $0^{th}$
\end_inset

 feature/intercept), for one training example is 
\begin_inset Formula 
\begin{eqnarray*}
h_{\theta}\left(x\right) & = & \left[\begin{array}{ccccc}
\theta_{0} & \theta_{1} & \theta_{2} & \ldots & \theta_{n}\end{array}\right]\left[\begin{array}{c}
1\\
x_{1}\\
x_{2}\\
\vdots\\
x_{n}
\end{array}\right]\\
 & = & \boldsymbol{\theta}^{T}\mathbf{x}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Remark*
For all 
\begin_inset Formula $m$
\end_inset

 training examples, we use the notation
\begin_inset Formula 
\begin{eqnarray*}
X & := & \left[\begin{array}{cccc}
x_{0}^{\left(1\right)} & x_{1}^{\left(1\right)} & \cdots & x_{n}^{\left(1\right)}\\
x_{0}^{\left(2\right)} & x_{1}^{\left(2\right)} & \cdots & x_{n}^{\left(2\right)}\\
\vdots & \vdots & \ddots & \vdots\\
x_{0}^{\left(m\right)} & x_{1}^{\left(m\right)} & \cdots & x_{n}^{\left(m\right)}
\end{array}\right]\\
h_{\theta}\left(X\right) & = & X\boldsymbol{\theta}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $x_{0}^{\left(i\right)}=1,\forall i=1,...,m$
\end_inset

.
\end_layout

\begin_layout Definition
The 
\series bold
design matrix 
\series default
of a training example is given by the matrix 
\begin_inset Formula $X\in\mathbb{R}^{m,n+1}$
\end_inset

 above.
\end_layout

\begin_layout Subsection
Evaluation: Cost function for multivariate linear regression
\end_layout

\begin_layout Standard
\begin_inset Formula $J:\mathbb{R}^{n+1}\longrightarrow\mathbb{R}$
\end_inset

 where
\begin_inset Formula 
\begin{eqnarray}
J\left(\theta\right) & = & \frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}\nonumber \\
 & = & \frac{1}{2m}\left[X\boldsymbol{\theta}-\mathbf{y}\right]^{T}\left[X\boldsymbol{\theta}-\mathbf{y}\right]\label{eq:cost-function}
\end{eqnarray}

\end_inset

for 
\begin_inset Formula $\mathbf{y}\in\mathbb{R}^{m}$
\end_inset

, 
\begin_inset Formula $\boldsymbol{\theta}\in\mathbb{R}^{n+1},$
\end_inset

 
\begin_inset Formula $X\in\mathbb{R}^{n\times n}$
\end_inset

.
\end_layout

\begin_layout Subsection
Optimisation: Gradient Descent for multivariate linear regression
\end_layout

\begin_layout Standard
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Algorithm

\series bold
Gradient Descent 
\end_layout

\begin_layout Algorithm
repeat until convergence {
\end_layout

\begin_layout Algorithm

\emph on
\begin_inset Formula $\boldsymbol{\theta}:=\boldsymbol{\theta}-\alpha\nabla J\left(\boldsymbol{\theta}\right)$
\end_inset


\end_layout

\begin_layout Algorithm

\emph on
}
\end_layout

\begin_layout Algorithm

\shape up
\emph on
for 
\begin_inset Formula $\alpha$
\end_inset

 some (positive) 
\series bold
learning
\series default
 
\series bold
rate
\series default
 (aka.
 step size) and some initial guess for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
\end_layout

\begin_layout Remark*
\begin_inset Formula $\nabla J\left(\boldsymbol{\theta}\right)=\left[\frac{\partial J\left(\boldsymbol{\theta}\right)}{\partial\theta_{0}},\frac{\partial J\left(\boldsymbol{\theta}\right)}{\partial\theta_{1}},...,\frac{\partial J\left(\boldsymbol{\theta}\right)}{\partial\theta_{n}}\right]^{T}$
\end_inset

 where 
\begin_inset Formula $\frac{\partial J\left(\boldsymbol{\theta}\right)}{\partial\theta_{j}}=\frac{1}{m}\sum_{i=1}^{m}x_{j}^{\left(i\right)}\left(h_{\boldsymbol{\theta}}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)$
\end_inset

.
 Equally 
\begin_inset Formula $\frac{\partial J\left(\boldsymbol{\theta}\right)}{\partial\theta_{j}}=\frac{1}{m}\mathbf{x}_{j}^{T}\left(X\boldsymbol{\theta}-\mathbf{y}\right)$
\end_inset

, so 
\begin_inset Formula $\nabla J\left(\boldsymbol{\theta}\right)=\frac{1}{m}X^{T}\left(X\boldsymbol{\theta}-\mathbf{y}\right)$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{\theta}:=\boldsymbol{\theta}-\frac{\alpha}{m}X^{T}\left(X\boldsymbol{\theta}-\mathbf{y}\right)$
\end_inset

; representing sums as a matrix is very useful, as there exist highly optimised
 libraries in performing matrix multiplication on a computer (e.g.
 the multiplication steps/updates can be computed in parallel).
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Gradient Descent in practice
\end_layout

\begin_layout Subsubsection
Feature Scaling
\end_layout

\begin_layout Standard

\series bold
\bar under
Problem
\end_layout

\begin_layout Standard
We should make sure all features have a 
\emph on
similar scale
\emph default
 so that gradient descent will converge more quickly.
 Consider the example in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1"

\end_inset

; plotting the contours for 
\begin_inset Formula $w_{1}:=\theta_{1}$
\end_inset

 vs.
 
\begin_inset Formula $w_{2}:=\theta_{2}$
\end_inset

 gives a very tall and thin shape due to the large range difference between
 the two parameters:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Images/Week2/Chapter3/elongated-ellipse.png
	lyxscale 15
	scale 15

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An elongated ellipse has direction of steepest descent almost perpendicular
 to the direction towards the minimum; the red gradient vector has a large
 component along the short axis of the ellipse (very steep descent), and
 a small component along the long axis of the ellipse - just the opposite
 to what we want.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
Solution
\end_layout

\begin_layout Standard

\series bold
\emph on
Feature Scaling:
\end_layout

\begin_layout Standard
As a rule of thumb, acceptable ranges for our input variables (features)
 are:
\end_layout

\begin_layout Itemize
\begin_inset Formula $-3\leq x_{j}^{\left(i\right)}\leq3$
\end_inset

 (no bigger)
\end_layout

\begin_layout Itemize
\begin_inset Formula $-\frac{1}{3}\leq x_{j}^{\left(i\right)}\leq\frac{1}{3}$
\end_inset

 (no smaller)
\end_layout

\begin_layout Standard
To achieve this, we can use 
\series bold
mean normalisation
\series default
 
\emph on
for each
\emph default
 training example 
\begin_inset Formula 
\[
\hat{x}_{j}=\frac{x_{j}-\mu}{s}
\]

\end_inset

where 
\begin_inset Formula $\mu:=$
\end_inset

average of all features (
\begin_inset Formula $x_{1},...,x_{n}$
\end_inset

) and 
\begin_inset Formula $s:=\max_{j=1,..,n}x_{j}-\min_{j=1,..,n}x_{j}$
\end_inset

.
\end_layout

\begin_layout Note*

\series bold
WARNING:
\series default
 We do not scale 
\begin_inset Formula $x_{0}=1$
\end_inset

.
\end_layout

\begin_layout Remark*
\begin_inset Formula $s$
\end_inset

 can also be replaced with the standard deviation.
\end_layout

\begin_layout Subsubsection
Debugging gradient descent/Judging convergence
\end_layout

\begin_layout Standard
If gradient descent is working then 
\begin_inset Formula $J\left(\boldsymbol{\theta}\right)$
\end_inset

 should 
\emph on
decrease after every iteration
\emph default
.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Images/Week2/Chapter3/debug-GD.png
	lyxscale 40
	scale 40

\end_inset


\begin_inset Graphics
	filename ../Images/Week2/Chapter3/too-big-alpha.png
	lyxscale 40
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:convergence-GD"

\end_inset

It appears that GD is working effectively on the left; convergence has been
 reached by approximately 300 iterations.
 If 
\begin_inset Formula $J\left(\boldsymbol{\theta}\right)$
\end_inset

 ever increases (e.g.
 right picture), it is likely to mean that we have overshot the minimum
 and so a smaller 
\begin_inset Formula $\alpha$
\end_inset

 is required.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Remark*
In general, it is hard to establish in advance how many iterations will
 be needed (anywhere from 
\begin_inset Formula $30-3,000,000$
\end_inset

 iterations).
 Typically declare convergence if 
\begin_inset Formula $J\left(\boldsymbol{\theta}\right)$
\end_inset

 decreases by less than 
\begin_inset Formula $10^{-3}$
\end_inset

 in one iteration (or via an observation like that in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:convergence-GD"

\end_inset

).
\end_layout

\begin_layout Subsubsection
Choosing a suitable learning rate 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_layout Standard
Typically run gradient descent on a range of 
\begin_inset Formula $\alpha$
\end_inset

 values, separating them by roughly threefold difference (e.g.
 0.001,0.003,0.01,0.03,0.1,...).
 Problems that can occur by choosing the wrong 
\begin_inset Formula $\alpha$
\end_inset

:
\end_layout

\begin_layout Itemize

\emph on
Too big:
\emph default
 GD can overshoot the minimum (figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:convergence-GD"

\end_inset

).
\end_layout

\begin_layout Itemize

\emph on
Too small: 
\emph default
GD can be slow to converge.
\end_layout

\begin_layout Subsubsection
Features and Polynomial Regression
\end_layout

\begin_layout Standard
Our hypothesis function need not be linear if that does not fit the data
 well.
 We can transform the behaviour (or curve) of our hypothesis function by
 making it a quadratic, cubic, square root function or any other form; e.g.
 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x_{1}$
\end_inset

 could become:
\end_layout

\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{1}^{2}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{1}^{2}+\theta_{3}x_{1}^{3}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}\sqrt{x_{1}}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}\log\left(x_{1}\right)$
\end_inset


\end_layout

\begin_layout Note*
The parameters (
\begin_inset Formula $\theta_{0},\theta_{1},...$
\end_inset

) still exist in a linear manner.
\end_layout

\begin_layout Standard
To map our old linear hypothesis and cost functions to these polynomial
 descriptions, the easy thing to do is set e.g.
 
\begin_inset Formula $x_{1}:=x_{1}$
\end_inset

, 
\begin_inset Formula $x_{2}:=\sqrt{x_{1}}$
\end_inset

 so that 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}\sqrt{x_{1}}=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}$
\end_inset

; note to scale, we scale only w.r.t.
 
\begin_inset Formula $x_{1}$
\end_inset

, so 
\begin_inset Formula $\sqrt{\frac{x_{1}}{\max_{j}x_{j}}}$
\end_inset

.
\end_layout

\begin_layout Remark*
The ability to enter features in non-linear forms makes linear regression
 a very powerful model that has much widespread use; indeed, non-linear
 relationships can often be closely approximated by Taylor series expansions.
\end_layout

\begin_layout Subsection
Alternative to gradient descent: normal equations
\end_layout

\begin_layout Standard
Iterative algorithms (such as gradient descent) takes steps to converge;
 normal equation solves for the optimum 
\begin_inset Formula $\theta$
\end_inset

 analytically.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="2">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top" width="6cm">
<column alignment="center" valignment="top" width="6cm">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Gradient Descent
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Normal Equations
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Need to choose 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No need to choose 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Choose the number of iterations (or can require many)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No iterations involved
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Works well when 
\begin_inset Formula $n$
\end_inset

 is large
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Slow if 
\begin_inset Formula $n$
\end_inset

 is very large (matrix inversion is typically 
\begin_inset Formula ${\cal O}\left(n^{3}\right)$
\end_inset

)
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Comparison of gradient descent and normal equations.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Remark*
Roughly when 
\begin_inset Formula $n$
\end_inset

 exceeds 
\begin_inset Formula $10,000$
\end_inset

 is a good point for changing between the two methods (
\begin_inset Quotes eld
\end_inset

Just a thought: could we combine the two methods/use a hybrid version at
 this point of intersection???
\begin_inset Quotes erd
\end_inset

)
\end_layout

\begin_layout Subsubsection
Cost function for normal equations
\end_layout

\begin_layout Standard
Taking the cost function as defined in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:cost-function"

\end_inset

, using the usual method (calculus) to find the minimum (see 
\begin_inset CommandInset href
LatexCommand href
name "Wikipedia"
target "https://en.wikipedia.org/wiki/Linear_least_squares_%28mathematics%29#Derivation_of_the_normal_equations"

\end_inset

 for a proof, where 
\begin_inset Formula $S\propto J\left(\boldsymbol{\theta}\right)$
\end_inset

), we see that 
\begin_inset Formula 
\[
\boldsymbol{\theta}=\left(X^{T}X\right)^{-1}X^{T}\mathbf{y}
\]

\end_inset


\end_layout

\begin_layout Note*
For any matrix 
\begin_inset Formula $Y\in\mathbb{R}^{m,n}$
\end_inset

, 
\begin_inset Formula $Y^{T}Y$
\end_inset

 is square and so invertible (assuming its determinant is non-zero).
\end_layout

\begin_layout Remark*
If 
\begin_inset Formula $X^{T}X$
\end_inset

 is not invertible, it is likely to be due to:
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
\emph on
Redundant features (aka.
 Collinearity)
\series default
\emph default
 Where two features are very closely related (i.e.
 they are linearly dependent).
\end_layout

\begin_layout Itemize

\series bold
\emph on
Too many features (e.g.
 
\series default
\emph default

\begin_inset Formula $m\leq n$
\end_inset


\series bold
\emph on
)
\series default
\emph default
 In this case, delete some features or use "regularisation" (let's you use
 lots of features for a small training set - see later notes).
 
\end_layout

\end_deeper
\begin_layout Subsection
R Example
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week2/Chapter3/prestige-preparation.png

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/andyandy/Desktop/Git/MyMOOCs/courses/Coursera/Machine_Learning/Images/Week2/Chapter3/linear-regression_R.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Linear regression in 
\series bold
R 
\series default
on the prestige dataset.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
