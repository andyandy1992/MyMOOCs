#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams-bytype
theorems-ams-extended-bytype
theorems-sec-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format pdf2
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 1cm
\headsep 1cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 3
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section

\lang british
Multivariate linear regression
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard

\lang british
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection

\lang british
Representation: Multiple Features
\end_layout

\begin_layout Notation*

\lang british
\begin_inset Formula $n:=$
\end_inset

number of features,
\end_layout

\begin_layout Notation*

\lang british
\begin_inset Formula $\mathbf{x}^{\left(i\right)}:=x^{\left(i\right)}=$
\end_inset

input (feature) of 
\begin_inset Formula $i^{th}$
\end_inset

 training example,
\end_layout

\begin_layout Notation*

\lang british
\begin_inset Formula $x_{j}^{\left(i\right)}:=$
\end_inset

value of feature 
\begin_inset Formula $j$
\end_inset

 in 
\begin_inset Formula $i^{th}$
\end_inset

 training example.
\end_layout

\begin_layout Remark*

\lang british
The more information we have about a data set, the better the predictions
 will be.
\end_layout

\begin_layout Example*

\lang british
Suppose now we have more features of a house on which to estimate the price
 of:
\end_layout

\begin_layout Example*
\noindent

\lang british
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\lang british
\begin_inset Graphics
	filename ../Images/Week2/Chapter1/Multiple features of a house.png
	lyxscale 80
	scale 80

\end_inset


\end_layout

\begin_layout Plain Layout

\lang british
\begin_inset Caption Standard

\begin_layout Plain Layout

\lang british
\begin_inset CommandInset label
LatexCommand label
name "fig:1"

\end_inset

Here 
\begin_inset Formula $n=4$
\end_inset

, 
\begin_inset Formula $x^{\left(2\right)}=\left[\begin{array}{c}
1416\\
3\\
2\\
40
\end{array}\right]$
\end_inset

 and so 
\begin_inset Formula $x_{2}^{\left(2\right)}=3$
\end_inset

, for instance.
\begin_inset Newline newline
\end_inset

So the hypothesis function extends to 
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}+\theta_{4}x_{4}=\left[\begin{array}{ccccc}
\theta_{0} & \theta_{1} & \theta_{2} & \theta_{3} & \theta_{4}\end{array}\right]\left[\begin{array}{c}
1\\
x_{1}\\
x_{2}\\
x_{3}\\
x_{4}
\end{array}\right]$
\end_inset

 for
\begin_inset Formula $\left[\begin{array}{ccccc}
\theta_{0} & \theta_{1} & \theta_{2} & \theta_{3} & \theta_{4}\end{array}\right],\:\left[\begin{array}{ccccc}
x_{0} & x_{1} & x_{2} & x_{3} & x_{4}\end{array}\right]^{T}\in\mathbb{R}^{5}$
\end_inset

 i.e.
 
\begin_inset Formula $x_{0}=1$
\end_inset

, the intercept.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Remark*

\lang british
The general hypothesis function for 
\begin_inset Formula $n$
\end_inset

-features (excluding the additional 
\begin_inset Formula $0^{th}$
\end_inset

 feature/intercept), for one training example is 
\begin_inset Formula 
\begin{eqnarray*}
h_{\theta}\left(x\right) & = & \left[\begin{array}{ccccc}
\theta_{0} & \theta_{1} & \theta_{2} & \ldots & \theta_{n}\end{array}\right]\left[\begin{array}{c}
1\\
x_{1}\\
x_{2}\\
\vdots\\
x_{n}
\end{array}\right]\\
 & = & \boldsymbol{\theta}^{T}\mathbf{x}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Remark*

\lang british
For all 
\begin_inset Formula $m$
\end_inset

 training examples, we use the notation
\begin_inset Formula 
\begin{eqnarray*}
X & := & \left[\begin{array}{cccc}
x_{0}^{\left(1\right)} & x_{1}^{\left(1\right)} & \cdots & x_{n}^{\left(1\right)}\\
x_{0}^{\left(2\right)} & x_{1}^{\left(2\right)} & \cdots & x_{n}^{\left(2\right)}\\
\vdots & \vdots & \ddots & \vdots\\
x_{0}^{\left(m\right)} & x_{1}^{\left(m\right)} & \cdots & x_{n}^{\left(m\right)}
\end{array}\right]\\
h_{\theta}\left(X\right) & = & X\boldsymbol{\theta}
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $x_{0}^{\left(i\right)}=1,\forall i=1,...,m$
\end_inset

 
\end_layout

\begin_layout Subsection

\lang british
Evaluation: Cost function for multivariate linear regression
\end_layout

\begin_layout Standard

\lang british
\begin_inset Formula 
\begin{eqnarray*}
J\left(\theta\right) & = & \frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}\\
 & = & \frac{1}{2m}\left[X\boldsymbol{\theta}-\mathbf{y}\right]^{T}\left[X\boldsymbol{\theta}-\mathbf{y}\right]
\end{eqnarray*}

\end_inset

for 
\begin_inset Formula $\mathbf{y}\in\mathbb{R}^{m}$
\end_inset

 the vector of all 
\begin_inset Formula $y$
\end_inset

 values.
\end_layout

\begin_layout Subsection

\lang british
Optimisation: Gradient Descent for multivariate linear regression
\end_layout

\begin_layout Standard

\lang british
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
status open

\begin_layout Algorithm

\series bold
\lang british
Gradient Descent 
\end_layout

\begin_layout Algorithm

\lang british
repeat until convergence {
\end_layout

\begin_layout Algorithm

\emph on
\lang british
\begin_inset Formula $\boldsymbol{\theta}:=\boldsymbol{\theta}-\alpha\nabla J\left(\boldsymbol{\theta}\right)$
\end_inset


\end_layout

\begin_layout Algorithm

\emph on
\lang british
}
\end_layout

\begin_layout Algorithm

\shape up
\emph on
\lang british
for 
\begin_inset Formula $\alpha$
\end_inset

 some (positive) 
\series bold
learning
\series default
 
\series bold
rate
\series default
 (aka.
 step size) and some initial guess for 
\begin_inset Formula $\boldsymbol{\theta}$
\end_inset

.
\end_layout

\begin_layout Remark*

\lang british
\begin_inset Formula $\nabla J\left(\boldsymbol{\theta}\right)=\left[\frac{\partial J\left(\boldsymbol{\theta}\right)}{\partial\theta_{0}},\frac{\partial J\left(\boldsymbol{\theta}\right)}{\partial\theta_{1}},...,\frac{\partial J\left(\boldsymbol{\theta}\right)}{\partial\theta_{n}}\right]^{T}$
\end_inset

 where 
\begin_inset Formula $\frac{\partial J\left(\boldsymbol{\theta}\right)}{\partial\theta_{j}}=\frac{1}{m}\sum_{i=1}^{m}x_{j}^{\left(i\right)}\left(h_{\boldsymbol{\theta}}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)$
\end_inset

.
 Equally 
\begin_inset Formula $\frac{\partial J\left(\boldsymbol{\theta}\right)}{\partial\theta_{j}}=\frac{1}{m}\mathbf{x}_{j}^{T}\left(X\boldsymbol{\theta}-\mathbf{y}\right)$
\end_inset

, so 
\begin_inset Formula $\nabla J\left(\boldsymbol{\theta}\right)=\frac{1}{m}X^{T}\left(X\boldsymbol{\theta}-\mathbf{y}\right)$
\end_inset

 and 
\begin_inset Formula $\boldsymbol{\theta}:=\boldsymbol{\theta}-\frac{\alpha}{m}X^{T}\left(X\boldsymbol{\theta}-\mathbf{y}\right)$
\end_inset

; representing sums as a matrix is very useful, as there exist highly optimised
 libraries in performing matrix multiplication on a computer (e.g.
 the multiplication steps/updates can be computed in parallel).
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Gradient Descent in practice
\end_layout

\begin_layout Subsubsection
Feature Scaling
\end_layout

\begin_layout Standard
We should make sure all features have a similar scale so that gradient descent
 will converge more quickly.
 Consider the example in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1"

\end_inset

; plotting the contours for 
\begin_inset Formula $\theta_{1}$
\end_inset

 vs.
 
\begin_inset Formula $\theta_{2}$
\end_inset

 gives a very tall and thin shape due to the large range difference between
 the two parameters:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../Images/Week2/Chapter1/FeatureScaling.png
	lyxscale 60
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_body
\end_document
